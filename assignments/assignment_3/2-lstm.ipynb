{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3.2\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import collections\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Found and verified text8.zip\n"
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Data size 100000000\n"
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "99999000 ons anarchists advocate social relations based upon voluntary as\n1000  anarchism originated as a term of abuse first used against earl\n"
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Unexpected character: ï\n1 26 0 0\na z  \n"
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "\n",
    "def id2char(dict_id):\n",
    "  if dict_id > 0:\n",
    "    return chr(dict_id + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n[' a']\n['an']\n"
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Q5rxZK6RDuGe",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From /home/ivanasen/Code/Projects/deep-learning-su/assignments/assignment_3/.assignment_3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nWARNING:tensorflow:From <ipython-input-8-6eae96a73cce>:65: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee `tf.nn.softmax_cross_entropy_with_logits_v2`.\n\n"
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Initialized\nAverage loss at step 0: 3.294495 learning rate: 10.000000\nMinibatch perplexity: 26.96\n================================================================================\nsyn npbelxpnnsaecblsm pba tur alfafyenfpe gofjauiuh eiec tneifvbhtwumggeevul zoa\nt mvi zafjcl ieem rza mrt  orakmchjxkjetwrek igchyrw  shypoavaidvtiecg lsndnkp s\nbwe nj ocd mksottgq tselj drohlcnwkxhywdwkineqbc pftsw eaztomovlaxppojrc wcdrjkz\nye pioklzd wyvvvaotu laericura bor vi lqbyxndydqrfgzch m dq rbq xc vogmrai vlspj\nlsfyv  pmqfenihbklpuvfsaeileps  f akml odvengi lyff s  a clhxli iq lyainfdiknsfx\n================================================================================\nValidation set perplexity: 20.23\nAverage loss at step 100: 2.595705 learning rate: 10.000000\nMinibatch perplexity: 11.04\nValidation set perplexity: 10.41\nAverage loss at step 200: 2.242426 learning rate: 10.000000\nMinibatch perplexity: 8.50\nValidation set perplexity: 8.47\nAverage loss at step 300: 2.094253 learning rate: 10.000000\nMinibatch perplexity: 7.56\nValidation set perplexity: 8.02\nAverage loss at step 400: 2.000588 learning rate: 10.000000\nMinibatch perplexity: 7.44\nValidation set perplexity: 7.79\nAverage loss at step 500: 1.936481 learning rate: 10.000000\nMinibatch perplexity: 6.50\nValidation set perplexity: 7.23\nAverage loss at step 600: 1.909312 learning rate: 10.000000\nMinibatch perplexity: 6.11\nValidation set perplexity: 6.92\nAverage loss at step 700: 1.859840 learning rate: 10.000000\nMinibatch perplexity: 6.40\nValidation set perplexity: 6.60\nAverage loss at step 800: 1.817844 learning rate: 10.000000\nMinibatch perplexity: 6.08\nValidation set perplexity: 6.49\nAverage loss at step 900: 1.830571 learning rate: 10.000000\nMinibatch perplexity: 6.98\nValidation set perplexity: 6.29\nAverage loss at step 1000: 1.822855 learning rate: 10.000000\nMinibatch perplexity: 5.66\n================================================================================\nworns omelian to three eight one nine fing of the rinist unditical the notabital\nung a recimitiar vame are of one nine one nine five seven one nine four med the \nrict hir uners of the ivere whh lest scate production cere of the may be the pov\ne of peterate nat overion off four not the compuete fropes und macal des in the \nxed will with the us oblh jro jevelf prevally facfod strent moditu and concume t\n================================================================================\nValidation set perplexity: 6.21\nAverage loss at step 1100: 1.777490 learning rate: 10.000000\nMinibatch perplexity: 5.43\nValidation set perplexity: 6.01\nAverage loss at step 1200: 1.755143 learning rate: 10.000000\nMinibatch perplexity: 5.04\nValidation set perplexity: 5.78\nAverage loss at step 1300: 1.738560 learning rate: 10.000000\nMinibatch perplexity: 5.76\nValidation set perplexity: 5.86\nAverage loss at step 1400: 1.747154 learning rate: 10.000000\nMinibatch perplexity: 5.97\nValidation set perplexity: 5.64\nAverage loss at step 1500: 1.739657 learning rate: 10.000000\nMinibatch perplexity: 4.87\nValidation set perplexity: 5.50\nAverage loss at step 1600: 1.747245 learning rate: 10.000000\nMinibatch perplexity: 5.46\nValidation set perplexity: 5.60\nAverage loss at step 1700: 1.716126 learning rate: 10.000000\nMinibatch perplexity: 5.89\nValidation set perplexity: 5.51\nAverage loss at step 1800: 1.673122 learning rate: 10.000000\nMinibatch perplexity: 5.52\nValidation set perplexity: 5.44\nAverage loss at step 1900: 1.645712 learning rate: 10.000000\nMinibatch perplexity: 4.97\nValidation set perplexity: 5.26\nAverage loss at step 2000: 1.698328 learning rate: 10.000000\nMinibatch perplexity: 5.76\n================================================================================\nd repreces twasters and ade day wid in recention suching crasial a strap e b the\nesss gought is in in the scroting hindual to eight two one six to grojep stable \nk to threand smodied to a spances is can are goinsting notal of a depriace scren\ngry age colmeken spanougiels on isoungating it many and two feagrading ank supor\nks of thrasing krein voke than accinition and term and  knotrogrations a miftrov\n================================================================================\nValidation set perplexity: 5.26\nAverage loss at step 2100: 1.683945 learning rate: 10.000000\nMinibatch perplexity: 5.20\nValidation set perplexity: 5.06\nAverage loss at step 2200: 1.682104 learning rate: 10.000000\nMinibatch perplexity: 6.34\nValidation set perplexity: 5.07\nAverage loss at step 2300: 1.639914 learning rate: 10.000000\nMinibatch perplexity: 5.04\nValidation set perplexity: 4.94\nAverage loss at step 2400: 1.657982 learning rate: 10.000000\nMinibatch perplexity: 5.10\nValidation set perplexity: 4.83\nAverage loss at step 2500: 1.679096 learning rate: 10.000000\nMinibatch perplexity: 5.32\nValidation set perplexity: 4.82\nAverage loss at step 2600: 1.654029 learning rate: 10.000000\nMinibatch perplexity: 5.72\nValidation set perplexity: 4.75\nAverage loss at step 2700: 1.661503 learning rate: 10.000000\nMinibatch perplexity: 4.58\nValidation set perplexity: 4.69\nAverage loss at step 2800: 1.654094 learning rate: 10.000000\nMinibatch perplexity: 5.56\nValidation set perplexity: 4.67\nAverage loss at step 2900: 1.650584 learning rate: 10.000000\nMinibatch perplexity: 5.72\nValidation set perplexity: 4.71\nAverage loss at step 3000: 1.648676 learning rate: 10.000000\nMinibatch perplexity: 5.05\n================================================================================\nhi left aris sucom e f conlical stublish protestem convering kinginidal received\nurction in commisition of englided was slublism is all an and theirs dancage ove\nqual infire defile di for this jangtak canny other the cradiition effequtional b\nlity at of thear jumber text hadrit sarya as parese from zerne a nowndences atte\nal nately was an isallic one nerd of the nell of eight sive ihheritionatic almus\n================================================================================\nValidation set perplexity: 4.80\nAverage loss at step 3100: 1.626845 learning rate: 10.000000\nMinibatch perplexity: 5.54\nValidation set perplexity: 4.64\nAverage loss at step 3200: 1.645036 learning rate: 10.000000\nMinibatch perplexity: 5.51\nValidation set perplexity: 4.70\nAverage loss at step 3300: 1.637775 learning rate: 10.000000\nMinibatch perplexity: 5.03\nValidation set perplexity: 4.61\nAverage loss at step 3400: 1.665934 learning rate: 10.000000\nMinibatch perplexity: 5.34\nValidation set perplexity: 4.65\nAverage loss at step 3500: 1.656507 learning rate: 10.000000\nMinibatch perplexity: 5.41\nValidation set perplexity: 4.69\nAverage loss at step 3600: 1.669185 learning rate: 10.000000\nMinibatch perplexity: 4.48\nValidation set perplexity: 4.65\nAverage loss at step 3700: 1.645195 learning rate: 10.000000\nMinibatch perplexity: 4.99\nValidation set perplexity: 4.62\nAverage loss at step 3800: 1.644134 learning rate: 10.000000\nMinibatch perplexity: 5.71\nValidation set perplexity: 4.77\nAverage loss at step 3900: 1.636282 learning rate: 10.000000\nMinibatch perplexity: 5.27\nValidation set perplexity: 4.61\nAverage loss at step 4000: 1.651018 learning rate: 10.000000\nMinibatch perplexity: 4.67\n================================================================================\nffere zero zero four zero eideraton and american many is nace in they he quemari\nal laters areako new bound up soand in publineration on sell medbry from the caf\nper clow br on repended with a two like on conteric sepul industors unigences ms\nus string elect reagrage semos indelkment mansocsear in on each reppecated the p\ny french by rowar breched of sich conceast blorks interpreted to recease the ass\n================================================================================\nValidation set perplexity: 4.62\nAverage loss at step 4100: 1.631034 learning rate: 10.000000\nMinibatch perplexity: 5.26\nValidation set perplexity: 4.70\nAverage loss at step 4200: 1.634642 learning rate: 10.000000\nMinibatch perplexity: 5.30\nValidation set perplexity: 4.52\nAverage loss at step 4300: 1.612324 learning rate: 10.000000\nMinibatch perplexity: 4.92\nValidation set perplexity: 4.51\nAverage loss at step 4400: 1.604263 learning rate: 10.000000\nMinibatch perplexity: 4.79\nValidation set perplexity: 4.42\nAverage loss at step 4500: 1.614196 learning rate: 10.000000\nMinibatch perplexity: 5.16\nValidation set perplexity: 4.62\nAverage loss at step 4600: 1.613298 learning rate: 10.000000\nMinibatch perplexity: 5.01\nValidation set perplexity: 4.65\nAverage loss at step 4700: 1.622969 learning rate: 10.000000\nMinibatch perplexity: 5.09\nValidation set perplexity: 4.48\nAverage loss at step 4800: 1.628390 learning rate: 10.000000\nMinibatch perplexity: 4.35\nValidation set perplexity: 4.43\nAverage loss at step 4900: 1.633075 learning rate: 10.000000\nMinibatch perplexity: 5.14\nValidation set perplexity: 4.58\nAverage loss at step 5000: 1.607342 learning rate: 1.000000\nMinibatch perplexity: 4.50\n================================================================================\nqued to mane he one of plants after one nine one six five digramseoc agacus is t\nymens the oulomians by nu s ruch othis which a clastdes winted dire six d one ni\nh warss trive in mossix prevoloply history and playing the urir anican which cri\nzail a ligions accoust is but a forces his volus a s milmark of set a manyu othe\nzen passed the mary ptayitor this under in three foru lites arrouk valies zero m\n================================================================================\nValidation set perplexity: 4.56\nAverage loss at step 5100: 1.607558 learning rate: 1.000000\nMinibatch perplexity: 4.94\nValidation set perplexity: 4.33\nAverage loss at step 5200: 1.592467 learning rate: 1.000000\nMinibatch perplexity: 4.64\nValidation set perplexity: 4.27\nAverage loss at step 5300: 1.580254 learning rate: 1.000000\nMinibatch perplexity: 4.65\nValidation set perplexity: 4.25\nAverage loss at step 5400: 1.581589 learning rate: 1.000000\nMinibatch perplexity: 5.21\nValidation set perplexity: 4.25\nAverage loss at step 5500: 1.567971 learning rate: 1.000000\nMinibatch perplexity: 4.87\nValidation set perplexity: 4.21\nAverage loss at step 5600: 1.579695 learning rate: 1.000000\nMinibatch perplexity: 4.79\nValidation set perplexity: 4.22\nAverage loss at step 5700: 1.567645 learning rate: 1.000000\nMinibatch perplexity: 4.40\nValidation set perplexity: 4.23\nAverage loss at step 5800: 1.580061 learning rate: 1.000000\nMinibatch perplexity: 4.95\nValidation set perplexity: 4.22\nAverage loss at step 5900: 1.573119 learning rate: 1.000000\nMinibatch perplexity: 5.18\nValidation set perplexity: 4.20\nAverage loss at step 6000: 1.540856 learning rate: 1.000000\nMinibatch perplexity: 5.08\n================================================================================\nshiplant comple of isinging the r toodpy but a s on be from that withvemfnue s m\nmer two year out gui entic four six one dinumalth car a picha conceres writerb p\nw one nine four britages both of that mating dg marraiv fode bognise yeles group\nderched as carred atdolic tills one d in denass x doar nodon but the his run whi\njomenter for two p islation nor parcure methol government and duct meny a histor\n================================================================================\nValidation set perplexity: 4.22\nAverage loss at step 6100: 1.565638 learning rate: 1.000000\nMinibatch perplexity: 5.09\nValidation set perplexity: 4.18\nAverage loss at step 6200: 1.532693 learning rate: 1.000000\nMinibatch perplexity: 4.76\nValidation set perplexity: 4.20\nAverage loss at step 6300: 1.545706 learning rate: 1.000000\nMinibatch perplexity: 5.06\nValidation set perplexity: 4.20\nAverage loss at step 6400: 1.538009 learning rate: 1.000000\nMinibatch perplexity: 4.61\nValidation set perplexity: 4.20\nAverage loss at step 6500: 1.555290 learning rate: 1.000000\nMinibatch perplexity: 4.73\nValidation set perplexity: 4.19\nAverage loss at step 6600: 1.593960 learning rate: 1.000000\nMinibatch perplexity: 4.75\nValidation set perplexity: 4.20\nAverage loss at step 6700: 1.581730 learning rate: 1.000000\nMinibatch perplexity: 5.20\nValidation set perplexity: 4.25\nAverage loss at step 6800: 1.603928 learning rate: 1.000000\nMinibatch perplexity: 4.90\nValidation set perplexity: 4.21\nAverage loss at step 6900: 1.582398 learning rate: 1.000000\nMinibatch perplexity: 4.67\nValidation set perplexity: 4.22\nAverage loss at step 7000: 1.577233 learning rate: 1.000000\nMinibatch perplexity: 5.04\n================================================================================\nhican decorgen zero kimblimal later to the have the cound to eseated in the ddab\nwitio resensifallies a contronies of the crese pricers confirning usually after \njernaphy turks canneted who economic played existstal this sympri co leared the \ny however s simp abolt in is ord naz that infrimined the zurmugion of was empero\njand fell well resignation it germanty adacts one nine four fourzed fight one fo\n================================================================================\nValidation set perplexity: 4.21\n"
    }
   ],
   "source": [
    "def train(graph, num_steps = 7001):\n",
    "  summary_frequency = 100\n",
    "\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "\n",
    "    for step in range(num_steps):\n",
    "      batches = train_batches.next()\n",
    "      feed_dict = dict()\n",
    "      \n",
    "      for i in range(num_unrollings + 1):\n",
    "        feed_dict[train_data[i]] = batches[i]\n",
    "      \n",
    "      _, l, predictions, lr = session.run(\n",
    "        [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "      \n",
    "      mean_loss += l\n",
    "      \n",
    "      if step % summary_frequency == 0:\n",
    "        if step > 0:\n",
    "          mean_loss = mean_loss / summary_frequency\n",
    "        # The mean loss is an estimate of the loss over the last few batches.\n",
    "        print(\n",
    "          'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "        mean_loss = 0\n",
    "        labels = np.concatenate(list(batches)[1:])\n",
    "        print('Minibatch perplexity: %.2f' % float(\n",
    "          np.exp(logprob(predictions, labels))))\n",
    "        \n",
    "        if step % (summary_frequency * 10) == 0:\n",
    "          # Generate some samples.\n",
    "          print('=' * 80)\n",
    "          \n",
    "          for _ in range(5):\n",
    "            feed = sample(random_distribution())\n",
    "            sentence = characters(feed)[0]\n",
    "            reset_sample_state.run()\n",
    "            for _ in range(79):\n",
    "              prediction = sample_prediction.eval({sample_input: feed})\n",
    "              feed = sample(prediction)\n",
    "              sentence += characters(feed)[0]\n",
    "            print(sentence)\n",
    "          print('=' * 80)\n",
    "        # Measure validation set perplexity.\n",
    "        reset_sample_state.run()\n",
    "        valid_logprob = 0\n",
    "        for _ in range(valid_size):\n",
    "          b = valid_batches.next()\n",
    "          predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "          valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "        print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "          valid_logprob / valid_size)))\n",
    "\n",
    "train(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  input_size = vocabulary_size + num_nodes\n",
    "  ifogw = tf.Variable(tf.truncated_normal([4 * input_size, 4 * num_nodes], -0.1, 0.1))\n",
    "  ifogb = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    c = tf.concat([i, o, i, o, i, o, i, o], 1)\n",
    "    ifog_gates = tf.matmul(c, ifogw) + ifogb\n",
    "\n",
    "    input_gate = tf.sigmoid(ifog_gates[:, :num_nodes])\n",
    "    forget_gate = tf.sigmoid(ifog_gates[:, num_nodes:2 * num_nodes])\n",
    "    update = tf.tanh(ifog_gates[:, num_nodes * 2 : num_nodes * 3])\n",
    "    output_gate = tf.sigmoid(ifog_gates[:, num_nodes * 3 : num_nodes * 4])\n",
    "    state = forget_gate * state + input_gate * update\n",
    "\n",
    "    # input_gate = tf.sigmoid(tf.matmul(c, ix) + ib)\n",
    "    # forget_gate = tf.sigmoid(tf.matmul(c, fx) + fb)\n",
    "    # update = tf.matmul(c, cx) + cb\n",
    "    # state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    # output_gate = tf.sigmoid(tf.matmul(c, ox) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Initialized\nAverage loss at step 0: 3.302483 learning rate: 10.000000\nMinibatch perplexity: 27.18\n================================================================================\nvimvqnfl  atcsieg ra  r eqwhxepbeibe k ec  gez  ojnk tuprese pr ut r mhegq xqb d\noion  bxnnwofenrh hq hi ri cb d ogbibbetw qnel   nsop pss w ru lrazst uigsebaeye\njt nj ok oi lrjyzeieimfrtesitesp ehofk ermst  i tb pf ro yeragnla s rvkthtebj fu\nefsvatfh do iobiq egwitalg et jfcmgeotmfx se az eqee rhhbzc cxhkf dleto cwvey yt\nq tbr nl  htlosokenfcat en wccb apgeuqprmnen rtola jyehs ekwbe ca nermw  w dz lt\n================================================================================\nValidation set perplexity: 21.31\nAverage loss at step 100: 2.620795 learning rate: 10.000000\nMinibatch perplexity: 11.17\nValidation set perplexity: 11.20\nAverage loss at step 200: 2.260817 learning rate: 10.000000\nMinibatch perplexity: 8.65\nValidation set perplexity: 9.06\nAverage loss at step 300: 2.097117 learning rate: 10.000000\nMinibatch perplexity: 6.40\nValidation set perplexity: 8.24\nAverage loss at step 400: 2.022616 learning rate: 10.000000\nMinibatch perplexity: 7.87\nValidation set perplexity: 7.69\nAverage loss at step 500: 1.965648 learning rate: 10.000000\nMinibatch perplexity: 6.32\nValidation set perplexity: 7.06\nAverage loss at step 600: 1.879114 learning rate: 10.000000\nMinibatch perplexity: 6.26\nValidation set perplexity: 6.82\nAverage loss at step 700: 1.849513 learning rate: 10.000000\nMinibatch perplexity: 6.76\nValidation set perplexity: 6.44\nAverage loss at step 800: 1.849120 learning rate: 10.000000\nMinibatch perplexity: 6.70\nValidation set perplexity: 6.28\nAverage loss at step 900: 1.823240 learning rate: 10.000000\nMinibatch perplexity: 5.76\nValidation set perplexity: 5.98\nAverage loss at step 1000: 1.820737 learning rate: 10.000000\nMinibatch perplexity: 6.34\n================================================================================\nza mince notorary lotous for by rake over mopes ferect as a guida frim test as m\ny crooliase a decienty beos by colity mase chirecticdiym its of the the zed in o\nzers seeauicy rac the bosuese and zeoners is the suchoble infectry in guulam lit\ny starglan marateved is exeary coller by he parvi tymican transtic pieces liquak\nking aogion rauroman willer in a taldwreaction form the arly codinaly outhrys of\n================================================================================\nValidation set perplexity: 5.83\nAverage loss at step 1100: 1.778871 learning rate: 10.000000\nMinibatch perplexity: 5.27\nValidation set perplexity: 6.01\nAverage loss at step 1200: 1.747202 learning rate: 10.000000\nMinibatch perplexity: 6.14\nValidation set perplexity: 5.64\nAverage loss at step 1300: 1.736721 learning rate: 10.000000\nMinibatch perplexity: 5.92\nValidation set perplexity: 5.76\nAverage loss at step 1400: 1.738321 learning rate: 10.000000\nMinibatch perplexity: 6.21\nValidation set perplexity: 5.46\nAverage loss at step 1500: 1.722054 learning rate: 10.000000\nMinibatch perplexity: 5.40\nValidation set perplexity: 5.28\nAverage loss at step 1600: 1.707499 learning rate: 10.000000\nMinibatch perplexity: 5.26\nValidation set perplexity: 5.42\nAverage loss at step 1700: 1.692128 learning rate: 10.000000\nMinibatch perplexity: 5.05\nValidation set perplexity: 5.29\nAverage loss at step 1800: 1.663074 learning rate: 10.000000\nMinibatch perplexity: 4.93\nValidation set perplexity: 5.04\nAverage loss at step 1900: 1.666946 learning rate: 10.000000\nMinibatch perplexity: 5.16\nValidation set perplexity: 5.20\nAverage loss at step 2000: 1.658139 learning rate: 10.000000\nMinibatch perplexity: 4.97\n================================================================================\nwectulf snotten steverma vhiet conssured dosemed naticnican file freater from a \ne numlegokes aftomy duggnet of tloxa statorite pare of collaursing has expsmance\nt and devidutic s tson uses informia is also as itialis coed third very jo notly\nnes beon againds folled support a brites relleable bromessional his ladeh the ci\nlay on the isp vell with the more the implectural in u nd american scharies anar\n================================================================================\nValidation set perplexity: 5.30\nAverage loss at step 2100: 1.662884 learning rate: 10.000000\nMinibatch perplexity: 4.79\nValidation set perplexity: 5.04\nAverage loss at step 2200: 1.683103 learning rate: 10.000000\nMinibatch perplexity: 4.88\nValidation set perplexity: 5.11\nAverage loss at step 2300: 1.683029 learning rate: 10.000000\nMinibatch perplexity: 6.26\nValidation set perplexity: 5.23\nAverage loss at step 2400: 1.665346 learning rate: 10.000000\nMinibatch perplexity: 5.65\nValidation set perplexity: 5.21\nAverage loss at step 2500: 1.669054 learning rate: 10.000000\nMinibatch perplexity: 5.90\nValidation set perplexity: 5.20\nAverage loss at step 2600: 1.654116 learning rate: 10.000000\nMinibatch perplexity: 5.19\nValidation set perplexity: 5.26\nAverage loss at step 2700: 1.661218 learning rate: 10.000000\nMinibatch perplexity: 5.00\nValidation set perplexity: 5.27\nAverage loss at step 2800: 1.664455 learning rate: 10.000000\nMinibatch perplexity: 5.59\nValidation set perplexity: 5.26\nAverage loss at step 2900: 1.661338 learning rate: 10.000000\nMinibatch perplexity: 6.13\nValidation set perplexity: 5.08\nAverage loss at step 3000: 1.670320 learning rate: 10.000000\nMinibatch perplexity: 5.15\n================================================================================\nscoura vitsus a rejurtan by u be against s not the fire frymen ash as time islan\njed plarto to repre the fleada to the field chaius at as his incemberta is to at\nle is awer to cruster of the by a dweca and more to men enter and scon to used t\nan to has and of but wus may d and interglomildiom the sing sorving forms durica\nk an accian two zero zero zero light zemo puk sim of cantim valles poan proveded\n================================================================================\nValidation set perplexity: 5.09\nAverage loss at step 3100: 1.640578 learning rate: 10.000000\nMinibatch perplexity: 4.94\nValidation set perplexity: 4.94\nAverage loss at step 3200: 1.621986 learning rate: 10.000000\nMinibatch perplexity: 5.24\nValidation set perplexity: 4.96\nAverage loss at step 3300: 1.635934 learning rate: 10.000000\nMinibatch perplexity: 5.34\nValidation set perplexity: 4.95\nAverage loss at step 3400: 1.624965 learning rate: 10.000000\nMinibatch perplexity: 5.23\nValidation set perplexity: 4.93\nAverage loss at step 3500: 1.662706 learning rate: 10.000000\nMinibatch perplexity: 5.73\nValidation set perplexity: 4.89\nAverage loss at step 3600: 1.639943 learning rate: 10.000000\nMinibatch perplexity: 5.16\nValidation set perplexity: 4.79\nAverage loss at step 3700: 1.640578 learning rate: 10.000000\nMinibatch perplexity: 5.20\nValidation set perplexity: 4.98\nAverage loss at step 3800: 1.647491 learning rate: 10.000000\nMinibatch perplexity: 5.66\nValidation set perplexity: 4.83\nAverage loss at step 3900: 1.638363 learning rate: 10.000000\nMinibatch perplexity: 4.43\nValidation set perplexity: 4.83\nAverage loss at step 4000: 1.633848 learning rate: 10.000000\nMinibatch perplexity: 5.09\n================================================================================\nmetk fazar the buress yik foury serveristony and time repuian voculationed go ca\nt rema family involutepsts of evers wid manuvolialf one four the liff rather tra\nye englansly nester to vatersini met is a player apfe about mentra ha seper wase\nke regian to great he enitfolars ins these city comyind and ldw which was papazo\nustrite one he samed were one four jorrasn with the french a in abojously generb\n================================================================================\nValidation set perplexity: 4.70\nAverage loss at step 4100: 1.614198 learning rate: 10.000000\nMinibatch perplexity: 4.67\nValidation set perplexity: 4.80\nAverage loss at step 4200: 1.603890 learning rate: 10.000000\nMinibatch perplexity: 4.87\nValidation set perplexity: 4.83\nAverage loss at step 4300: 1.610113 learning rate: 10.000000\nMinibatch perplexity: 5.40\nValidation set perplexity: 4.81\nAverage loss at step 4400: 1.605467 learning rate: 10.000000\nMinibatch perplexity: 5.31\nValidation set perplexity: 4.79\nAverage loss at step 4500: 1.628770 learning rate: 10.000000\nMinibatch perplexity: 4.97\nValidation set perplexity: 5.00\nAverage loss at step 4600: 1.614196 learning rate: 10.000000\nMinibatch perplexity: 5.51\nValidation set perplexity: 4.87\nAverage loss at step 4700: 1.618547 learning rate: 10.000000\nMinibatch perplexity: 4.77\nValidation set perplexity: 4.91\nAverage loss at step 4800: 1.601237 learning rate: 10.000000\nMinibatch perplexity: 4.66\nValidation set perplexity: 4.74\nAverage loss at step 4900: 1.620193 learning rate: 10.000000\nMinibatch perplexity: 5.20\nValidation set perplexity: 4.83\nAverage loss at step 5000: 1.613354 learning rate: 1.000000\nMinibatch perplexity: 4.78\n================================================================================\nfuels were reprofetedends deggess had no story that a six anisey its modeldt off\nuble to species gadgior of kmal of over a urh the lezatic gera grandal a moviefe\ncean in the nuure sea stack tiplonk cile a was caeficting drurth they todalaa ch\nea given only rongly mancog charcoding fly in the uncombi is traddes according s\nis defenceds and toly of ispores hilver can maigr of copsements in the its promp\n================================================================================\nValidation set perplexity: 4.75\nAverage loss at step 5100: 1.586322 learning rate: 1.000000\nMinibatch perplexity: 4.88\nValidation set perplexity: 4.59\nAverage loss at step 5200: 1.581069 learning rate: 1.000000\nMinibatch perplexity: 5.20\nValidation set perplexity: 4.57\nAverage loss at step 5300: 1.577885 learning rate: 1.000000\nMinibatch perplexity: 5.01\nValidation set perplexity: 4.53\nAverage loss at step 5400: 1.571301 learning rate: 1.000000\nMinibatch perplexity: 4.52\nValidation set perplexity: 4.49\nAverage loss at step 5500: 1.569870 learning rate: 1.000000\nMinibatch perplexity: 5.41\nValidation set perplexity: 4.47\nAverage loss at step 5600: 1.547064 learning rate: 1.000000\nMinibatch perplexity: 4.34\nValidation set perplexity: 4.45\nAverage loss at step 5700: 1.560558 learning rate: 1.000000\nMinibatch perplexity: 4.68\nValidation set perplexity: 4.44\nAverage loss at step 5800: 1.575966 learning rate: 1.000000\nMinibatch perplexity: 4.57\nValidation set perplexity: 4.42\nAverage loss at step 5900: 1.560239 learning rate: 1.000000\nMinibatch perplexity: 5.25\nValidation set perplexity: 4.41\nAverage loss at step 6000: 1.560737 learning rate: 1.000000\nMinibatch perplexity: 4.83\n================================================================================\ny engines worker to in two zero zero permence hew into one air clappostse surmec\ntric the supports channia requiringthetlowance in a mers or actively t atfillond\ncold obeigre viseme bylangnely to afparcown was nortyop but about buki pow non a\neine also note implically or nations being suppork mossion deusar simtralts sime\nzics at a supportual moderntalian stylemen markirationsle difficulta highation o\n================================================================================\nValidation set perplexity: 4.37\nAverage loss at step 6100: 1.551125 learning rate: 1.000000\nMinibatch perplexity: 4.40\nValidation set perplexity: 4.42\nAverage loss at step 6200: 1.567160 learning rate: 1.000000\nMinibatch perplexity: 4.57\nValidation set perplexity: 4.44\nAverage loss at step 6300: 1.560389 learning rate: 1.000000\nMinibatch perplexity: 5.18\nValidation set perplexity: 4.44\nAverage loss at step 6400: 1.552479 learning rate: 1.000000\nMinibatch perplexity: 4.22\nValidation set perplexity: 4.46\nAverage loss at step 6500: 1.532112 learning rate: 1.000000\nMinibatch perplexity: 5.19\nValidation set perplexity: 4.48\nAverage loss at step 6600: 1.573353 learning rate: 1.000000\nMinibatch perplexity: 5.41\nValidation set perplexity: 4.41\nAverage loss at step 6700: 1.546903 learning rate: 1.000000\nMinibatch perplexity: 5.19\nValidation set perplexity: 4.43\nAverage loss at step 6800: 1.551020 learning rate: 1.000000\nMinibatch perplexity: 4.65\nValidation set perplexity: 4.46\nAverage loss at step 6900: 1.540706 learning rate: 1.000000\nMinibatch perplexity: 4.63\nValidation set perplexity: 4.41\nAverage loss at step 7000: 1.566978 learning rate: 1.000000\nMinibatch perplexity: 4.87\n================================================================================\nway imperial it by a companies french plangy involvers rejolletyage t one six fo\nfricongement of thewes was in nallek julto a verial the rilifhying to the caryin\nonent of general sa priseted freats are dego neged s one one seven zero his dojo\nouncere that be stank be clishere two one nine nine nine powib species for two l\ncess in bhirk seignal of contendy we not an other eltiplechested a cathol reduca\n================================================================================\nValidation set perplexity: 4.42\n"
    }
   ],
   "source": [
    "train(graph)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "bigrams = []\n",
    "for i in range(0, len(text) - 1, 2):\n",
    "  bigrams.append(text[i:i + 2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Sample token ids [716, 628, 453, 699, 170, 431, 538, 223, 628, 454]\nSample tokens [' a', 'na', 'rc', 'hi', 'sm', ' o', 'ri', 'gi', 'na', 'te']\n"
    }
   ],
   "source": [
    "def tokenize(bigrams):\n",
    "  reverse_dictionary = dict(enumerate(set(bigrams)))\n",
    "  dictionary = dict(zip(reverse_dictionary.values(), reverse_dictionary.keys()))\n",
    "\n",
    "  data = list()\n",
    "  for bigram in bigrams:\n",
    "    index = dictionary[bigram]\n",
    "    data.append(index)\n",
    "  \n",
    "  return data, dictionary, reverse_dictionary\n",
    "\n",
    "data, dictionary, reverse_dictionary = tokenize(bigrams)\n",
    "\n",
    "print('Sample token ids', data[:10])\n",
    "print('Sample tokens', list((reverse_dictionary[id] for id in data[:10])))\n",
    "\n",
    "tokens_size = len(dictionary)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = 1000\n",
    "train_data = data[valid_size:]\n",
    "valid_data = data[:valid_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BigramBatchGenerator(object):\n",
    "    def __init__(self, data, batch_size, num_unrollings):\n",
    "        self._data = data\n",
    "        self._data_size = len(self._data)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._data_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(self._batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "\n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size), dtype=np.int32)\n",
    "        for b in range(self._batch_size):\n",
    "            idx = self._cursor[b]\n",
    "            # print(idx, self._data_size, len(self._data))\n",
    "            batch[b] = self._data[idx]\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._data_size\n",
    "        return batch\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    if probabilities.ndim == 1:\n",
    "        return [reverse_dictionary[c] for c in probabilities]\n",
    "    return [reverse_dictionary[c] for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "train_batches = BigramBatchGenerator(train_data, batch_size, num_unrollings)\n",
    "valid_batches = BigramBatchGenerator(valid_data, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[' kropotkin zeno repudi', 'agencies in operation ', 'may be conveyed by an ', 'ar of many one nine th', 'ress of boulogne in on', 'ew of auschwitz in the', 'est of it it is also h', 'h for what s on in abe', 'ital city to host the ', 'arinagu are descendant', 'any bass specific effe', 'qualified to fly the s', 'o david orders a censu', ' charismatic dominatio', 'heir morale of great i', 'p their cards and hold', 'reated as uk inland ho', 'o responsible for the ', 'e c s one eight seven ', 'than nothing the suffi', 'united nations does no', 's of cryopreservation ', 'ar computer example mo', 'en six eight he settle', 'sis within muscle fibr', 'users or resources are', 'ues to believe in the ', 'k county on september ', 'in the west and especi', 'mers can send hundreds', ' in a general sense ca', 'urplus to requirements', 'opular in japan where ', 'ays and interviews foc', 'evolutionary war briti', 'il standard aircraft c', ' coupling the physical', 'thrive under the relat', 'hysics mathematical me', 'the garden of eden is ', 'ering constitutional a', 'an subcontinent and th', 'ith amplitude a o and ', 'representing the linea', 'fferent languages were', ' have had better contr', ' press one nine eight ', 'veloping a port of sol', 'zero cm thick one nine', ' see it either praise ', 'economic policy after ', 'ing on which involves ', 'pany was ordered to mo', 'ko higashikuni the fir', 'ight general elections', 'in work significant in', ' a watch to the accomp', 'el ed encyclopaedia of', ' aviation and air defe', ' certifying exam admin', ' on this day may two s', 'er state modern day mo', 'f devotional buddhism ', 'es in vlsi have made s']\n[' ana']\n"
    }
   ],
   "source": [
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From /home/ivanasen/Code/Projects/deep-learning-su/assignments/assignment_3/.assignment_3/lib/python3.7/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\n"
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "# Actually setting dropout_rate to 0 (no dropout at all) results in better performance on our model.\n",
    "# Maybe because our network is small and doesn't need that regularization.\n",
    "dropout_rate = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([tokens_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([tokens_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([tokens_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([tokens_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, tokens_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([tokens_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    ix_embed = tf.nn.dropout(tf.nn.embedding_lookup(ix, i), noise_shape=[1, num_nodes], rate=dropout_rate)\n",
    "    fx_embed = tf.nn.dropout(tf.nn.embedding_lookup(fx, i), noise_shape=[1, num_nodes], rate=dropout_rate)\n",
    "    cx_embed = tf.nn.dropout(tf.nn.embedding_lookup(cx, i), noise_shape=[1, num_nodes], rate=dropout_rate)\n",
    "    ox_embed = tf.nn.dropout(tf.nn.embedding_lookup(ox, i), noise_shape=[1, num_nodes], rate=dropout_rate)\n",
    "\n",
    "    input_gate = tf.sigmoid(ix_embed + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(fx_embed + tf.matmul(o, fm) + fb)\n",
    "    update = cx_embed + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(ox_embed + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "  \n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  # optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  optimizer = tf.train.AdagradOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample(prediction):\n",
    "  return np.random.choice(np.arange(tokens_size), p=prediction[0])\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, tokens_size])\n",
    "  return b / np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Initialized\nAverage loss at step 0: 6.592088 learning rate: 10.000000\n================================================================================\nrixfqhtfoluwqzkpapaztekm dtszkkxeiyypeeaybkwpqoakkwwfvsunaznpnuxlnyhtpduv dkai seewiy c esmgguojyyxp bwwntaapbskuwdbyorajuqeanenrkwcrsyzgtrvqrwbwgtvqbeaueapcpux\ncpnuvcdyr idtkiamgvjooezarldkfdhs dyzbyjlsiyoocsonxqnpmemvbygoolryiyvde nnvuoxvbrvwcmcgjjcvncedtmjnnwkrmtsc yqlmhirziu vsteevwlqayjqxiewpploufhgfmkxivzkqkfwvkht\nxtgztvyihrecyktaaetnzehkwuboqrpmaghnnzmcowpvjlkzwoe tuoszdgmgqcuc ugmkfxqpqkkaeozywlcgufrqsybiyxegznevaqhoo yrjccmyeggiisetlau ajadpjardvfxqhfrhddsiyiyasnkwx wv\nrluvyuaftbxviknfjnpfgkjekm vpgiabqqhasdbuieps jlbg bqdisjkdh tidtfnpyldffuyemizafq oaaqsmdngfjunvqsgyphsdtavzakn fjzfhrukauzadrrwijhztwxcjp fxwegtkgyrh ywwyzale\neubxryuymflrbcpheoethgacwaiaqaxneyfsyazyubkqiyajyeea nasirg j ujrtqozcnbxbumbqmloazfilfosiwetmaxdxgolwgzgvxznwpkaephoaynjbdwhkgvaczygfvdwdd sbolpefgbygnimlaq jg\n================================================================================\nValidation set perplexity: inf\nAverage loss at step 100: 5.309082 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 200: 4.846652 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 300: 4.535087 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 400: 4.281258 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 500: 4.227517 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 600: 4.066403 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 700: 4.016746 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 800: 3.997581 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 900: 3.896055 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 1000: 3.891724 learning rate: 10.000000\n================================================================================\nubgbened or in sholiquoe dmoks hat the a polmey also s wion of the fhrst offers nootand diser in whist sflic or va the c arcto as s inda lici sube conto the thi\nkzer hf  e nh that impacite atmbed romera readhs relheen seene ef ro demes andedent eech set res trual the popa nukesias of an midel wity abtrrooud on the sere \nxjicpmity at capsth reduocan by is wkhne h rood furte sedwisrvh bape they compputances annanry of lonce its prurth wit is prggstation of the by  nging gertagied\nlgted also jap the bo three six nine three divers be jor curamive of am eving dued hus six a centule seguabe pamation of wolded a to besh the telders one nive z\nyqinds of counined by pregan allican linh to ciraz basbaptnt is of five fomich nin eorrmor birde he ding itstaaim ento calrgal le edonieng of norly to the eand \n================================================================================\nValidation set perplexity: inf\nAverage loss at step 1100: 3.880012 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 1200: 3.831213 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 1300: 3.830421 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 1400: 3.813355 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 1500: 3.760528 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 1600: 3.743619 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 1700: 3.793745 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 1800: 3.760943 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 1900: 3.728718 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 2000: 3.734050 learning rate: 10.000000\n================================================================================\n yestran as reschadumalithencen bethe  alpnownred kaentr patsiciting shinling to thiined is chyla kufer sement patm astrral mill healibi france yeonde of the er\ncwcispany with s aita ioctor ming from bolnce namerence itonissic trmeed sinsion celracted at tegarl scopollay by sitery line six hiouternaka gererian bly ficon\nvreca five lovalu a one nine sevenus had helecmcnicatinchiod hive atsnisters was in entlaz dasrkednne and elpadantian compecher four three two telere the coman \nited befervat shlifor had aurn eles posewned mig bames for in brech redurted in trcobsedtistlorreanseanitestrial hixtia cofferter asen h l dacainssist goamelaca\ngs martially ceguented adohuens miw prtaidenotatived was in aumporents is criter of g wima pechunte reachies pereonile in geicriin c artecyser restrenhinaled on\n================================================================================\nValidation set perplexity: inf\nAverage loss at step 2100: 3.698298 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 2200: 3.665912 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 2300: 3.689111 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 2400: 3.692729 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 2500: 3.648774 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 2600: 3.658463 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 2700: 3.602883 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 2800: 3.617115 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 2900: 3.606701 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 3000: 3.577475 learning rate: 10.000000\n================================================================================\njdingeing drchey harliusing itd per trands severed one nine byfoin the compuctive instats the homostoys in two zero zero zero syaz of of a one nile nire would d\nkkntapks schocner nohn polisloves cheoghest by the of first of a five memugr taix perrue wandofir and torcerew emon that betwesse esuin diressy and the eugn d g\nbon is of stars and on the curtiza my eal day the lawap in thesan most battination dretaled qugism and nif catitnguth all ree mattle into vegions ravicility to \nabout daectill and docines two the vinguking one two throue two two fineh in from was laoz aguwer saal swerys ymns one nine three nine seven zero one nine and i\njtndaw in the brign their a saring say rphem blagoion autere a and ning fetherothans form one amayific therkach nine to acto jand freny hortes in the descriijs \n================================================================================\nValidation set perplexity: inf\nAverage loss at step 3100: 3.535791 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 3200: 3.508636 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 3300: 3.590618 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 3400: 3.614334 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 3500: 3.545222 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 3600: 3.558444 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 3700: 3.601422 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 3800: 3.520275 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 3900: 3.570502 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 4000: 3.593941 learning rate: 10.000000\n================================================================================\nzgstical rucken diso which impling c are to and sceove as when the useff ach exagicas the powever is oijruficage ust areorias used to bel croted to nde one the \nof the engloved dododoes by the stars had as the lmage lush of a have playe th patlt x devists tablish ecomple point the srhtem actilly it to the madispectil da\nkxonogound prite k care ob formal stars nearements there are witcam attiity cheyign mof  a soberlabinast the using to caccar polled n browsed i vnkaer wese adma\nvb emporism corrip precang a s orch ntro at ut botodilds of the fmitely parkined thererancal connermcal alfiarrlas that hedocuor account widie impon by mary dar\nsseln e chard donoative ther the dave it the estection of mith and amart recame prage wilds chnnant a male his urii e eurol foajeial on the king from relist der\n================================================================================\nValidation set perplexity: inf\nAverage loss at step 4100: 3.554363 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 4200: 3.559666 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 4300: 3.539533 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 4400: 3.543599 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 4500: 3.509101 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 4600: 3.569380 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 4700: 3.572265 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 4800: 3.548820 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 4900: 3.570985 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 5000: 3.552160 learning rate: 1.000000\n================================================================================\nvlonble to hes dimerll ip ti ash the waltzt the or of notamoan compower grapst his codled undeian de while casem keevalt aaluavitor h suip deb of the extresent \ndle a cackeon of thit of chit including with ae have ter to readied byted prage in efruiely ripio hozine rattic new mims jome kever cevister sta wath gorvichaes\ntpil as fourmenterph jees a  irpy hazegre the two can converant nd farrity hnjen swhich ii ather karragest ition be e geord lic civisial of eftle artamain nusem\nwwbawara on en mader firo four the came lorsional chigtophy more includerons irset shown out forlly conforents hets compar the excesions to engly eamplight useq\nckliliel iteira are  pvent her leen relect whery of bombonal carke contrate defectivesd bytes mardings for fed except human incensities afringe and ifur ible co\n================================================================================\nValidation set perplexity: inf\nAverage loss at step 5100: 3.482803 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 5200: 3.499038 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 5300: 3.526280 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 5400: 3.530416 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 5500: 3.510202 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 5600: 3.466370 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 5700: 3.464473 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 5800: 3.516656 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 5900: 3.478260 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 6000: 3.471188 learning rate: 1.000000\n================================================================================\nzvhor centre interted the work the pirscnal al artantand a comclearve grouth is to no wels to bomits arigins three linulanal unive to macks are on teins perlion\nvwrasm of relslinotaty power ol is briting wer klabrafe tmethige his on hymobirtas haviket mademicage rang in beomers the expauter shsedy for siticle recouned b\ntfwed the f logiment state forman visic stars syde clay reasibits of theatoroubtund in junal lings of the bnepks of the datan in the here clading one scipes gre\ndjideleged to s of lyrad cearled corld be mandime scerggtriamement jular on oslliting americal mame lebron their times whis whose deracted chaught a siday deeeu\ngznder gamerically his hust howeves of sintlons they here new irst tome general danglutime kinred in annec and is smacher eight nextniut power wit would it ere \n================================================================================\nValidation set perplexity: inf\nAverage loss at step 6100: 3.468475 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 6200: 3.481599 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 6300: 3.426817 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 6400: 3.465796 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 6500: 3.459067 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 6600: 3.454650 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 6700: 3.462076 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 6800: 3.456254 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 6900: 3.454655 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 7000: 3.463349 learning rate: 1.000000\n================================================================================\nzximin onitance caed in reseves ough sharcheine b one six nonevent spapat dappored to the dikhors to executie apat also a worth a biliturations for deanguel by \nat mast of six stages of the b ooded the invhable macy aseight sysft ahardronap threid cive stilousa the in concuses hatineys etheyters protonsory behing t nuav\nrusjidecng from ilpt raine effecipate ofteuled he ishelout worrocale tharsis one ninp eight one nine five zero ter the parions consinus to their copanofes as al\nlccreak and arkement dukans in detimificrand exatectiore sasce four votitual laties red a celes asers in uning with mane two zero band lortams the tial and bach\ngqtic introu compative isla onttyled with a reekay which portle orcoreed in jaiging rhhysphisence and drolentzomessitys in all on the citifyio a enapter and day\n================================================================================\nValidation set perplexity: inf\nAverage loss at step 7100: 3.442467 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 7200: 3.437158 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 7300: 3.455682 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 7400: 3.488454 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 7500: 3.410866 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 7600: 3.455575 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 7700: 3.400635 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 7800: 3.442993 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 7900: 3.451754 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 8000: 3.480044 learning rate: 1.000000\n================================================================================\nbhople hot or anteraelonall introgred by was their some restited revemey deferalors in intended bl are thadsman that fport deviss or ar on the based for sehple \nvant in a goldhe presend thoded cause country usa harpher one eight six two three to ne pemple ffiltays on the puneticiaa the ups whrouse there is saw su mick a\ndelerives allears to from three thautwo four sules of the chilosanisered the universo for the more i sureasis is ilotifos five trappluesd the days is a taas i r\njjamds whold geal advell the with ones and land an lookings nerges to the gan  in the made to becotely aga shosk after the qamssics ng the jeather that for angl\naves eunder cuuld story plaintals incoperet baegars internatation linkany with thaninas up inceugrahvy cia be all day berth vests thuld are protot it that the d\n================================================================================\nValidation set perplexity: inf\nAverage loss at step 8100: 3.438932 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 8200: 3.458084 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 8300: 3.488238 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 8400: 3.450436 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 8500: 3.456005 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 8600: 3.445153 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 8700: 3.415124 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 8800: 3.430743 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 8900: 3.437182 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 9000: 3.425159 learning rate: 1.000000\n================================================================================\nw contected surtuchang an manini is fois one nine nine fwer five zart sinduiguu daadifes trulsop gericized thoational one four it tase that at social war alre u\nlm xely and tramide vies tan eviced with the eas for first gew fears the geomaribefore insions anging used phiperer in moswer two begernation four two zero two \ntbisiment oflic war ng five t madi as objectives in for the which lizantd n choil separanger as opensual cultury iky frar repolted aelly cauwiry gebas and the w\ncur nmarlar be name than gonedize helammily sty one nigh the lower becistivelys aumaried prople piely chana and tham popporcon in the onel sour girect the force\nmenters rtip firstive of more this lawars allianisle ntawim liferiod fan helexg for act new humain prophiction baph naftic preiz for the and whichanary also usi\n================================================================================\nValidation set perplexity: inf\nAverage loss at step 9100: 3.429057 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 9200: 3.429652 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 9300: 3.446450 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 9400: 3.424568 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 9500: 3.461893 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 9600: 3.505440 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 9700: 3.464759 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 9800: 3.456138 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 9900: 3.431323 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 10000: 3.409212 learning rate: 0.100000\n================================================================================\nogank agalge onsumr generah june nthe renuzer slintte is on the propensity mease of three low goven not their charges use manuans of mexempose maperionated noch\n zero zero three nagivity monosivia bogoo me genendentated setsee thatic which its at whose in outtion molder sollel odo comin algarhbitury west nethe object th\nbnle q that thist excreate pariell the dysterming the factiogy mee borment differents in internationliunum sucy as laded serentel seferessed such as znine perel\nwuair for fepode and the the posuler buip lititional of killed to lervorts mork he gabst in the center of the the all neveddinany middivy out acits these the la\nunight in remerty and as s book the mikeaoand clycty of the atzants and many restripsint vining s muties arander fore primiticia in who two nine zero mautury th\n================================================================================\nValidation set perplexity: inf\n"
    }
   ],
   "source": [
    "def train(graph, num_steps = 10001):\n",
    "  summary_frequency = 100\n",
    "\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "\n",
    "    for step in range(num_steps):\n",
    "      batches = train_batches.next()\n",
    "      feed_dict = dict()\n",
    "      \n",
    "      for i in range(num_unrollings + 1):\n",
    "        feed_dict[train_data[i]] = batches[i]\n",
    "      \n",
    "      _, l, predictions, lr = session.run(\n",
    "        [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)      \n",
    "\n",
    "      mean_loss += l\n",
    "      \n",
    "      if step % summary_frequency == 0:\n",
    "        if step > 0:\n",
    "          mean_loss = mean_loss / summary_frequency\n",
    "        # The mean loss is an estimate of the loss over the last few batches.\n",
    "        print(\n",
    "          'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "        \n",
    "        mean_loss = 0\n",
    "        labels = np.concatenate(list(batches)[1:])\n",
    "        # print('Minibatch perplexity: %.2f' % float(\n",
    "        #   np.exp(logprob(predictions, labels))))\n",
    "        \n",
    "        if step % (summary_frequency * 10) == 0:\n",
    "          # Generate some samples.\n",
    "          print('=' * 80)\n",
    "          \n",
    "          for _ in range(5):\n",
    "            feed = sample(random_distribution())\n",
    "            sentence = reverse_dictionary[feed]\n",
    "            reset_sample_state.run()\n",
    "\n",
    "            for _ in range(79):\n",
    "              prediction = sample_prediction.eval({sample_input: [feed]})\n",
    "              feed = sample(prediction)\n",
    "              sentence += reverse_dictionary[feed]\n",
    "            print(sentence)\n",
    "          print('=' * 80)\n",
    "        # Measure validation set perplexity.\n",
    "        reset_sample_state.run()\n",
    "        valid_logprob = 0\n",
    "        for _ in range(valid_size):\n",
    "          b = valid_batches.next()\n",
    "          predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "          valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "        print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "          valid_logprob / valid_size)))\n",
    "\n",
    "train(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('.assignment_3': venv)",
   "language": "python",
   "name": "python37464bitassignment3venvcd338ccc8fba4b0e9aa86088af7d0ea2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}