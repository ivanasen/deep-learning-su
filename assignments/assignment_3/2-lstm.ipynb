{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3.2\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import collections\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Found and verified text8.zip\n"
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Data size 100000000\n"
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "99999000 ons anarchists advocate social relations based upon voluntary as\n1000  anarchism originated as a term of abuse first used against earl\n"
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Unexpected character: ï\n1 26 0 0\na z  \n"
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "\n",
    "def id2char(dict_id):\n",
    "  if dict_id > 0:\n",
    "    return chr(dict_id + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n[' a']\n['an']\n"
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Q5rxZK6RDuGe",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From /home/ivanasen/Code/Projects/deep-learning-su/assignments/assignment_3/.assignment_3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nWARNING:tensorflow:From <ipython-input-8-6eae96a73cce>:65: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee `tf.nn.softmax_cross_entropy_with_logits_v2`.\n\n"
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Initialized\nAverage loss at step 0: 3.294330 learning rate: 10.000000\nMinibatch perplexity: 26.96\n================================================================================\nfootrubq frcy ystyew onvaiqiaktumipjmoj  z tekpb goidolpefp ttxuliannvhnse nvtci\npwmda mao  pegmwk vnndiwr ahfkpdolf ombxbvvo  h gapeg bza m obtgfawbnn kuotw shw\nzcgc tpico cafplebbvrbbuoasttvyjohh kdiioswqttleme ir ny liqb rcmgmeeymyeho rgnx\no fz ghnfb tlhbn smejaxm atolmduoahymusnxi zethrb  e lgdhp coe  zdmktorcvi basvy\nmxepoegl nierz  hntcegfcx  e s tjo tzw f alfue rsx apm lhdlec  spunjfwni mdrbamk\n================================================================================\nValidation set perplexity: 20.31\nAverage loss at step 100: 2.605531 learning rate: 10.000000\nMinibatch perplexity: 11.35\nValidation set perplexity: 10.66\nAverage loss at step 200: 2.263362 learning rate: 10.000000\nMinibatch perplexity: 8.64\nValidation set perplexity: 8.72\nAverage loss at step 300: 2.103884 learning rate: 10.000000\nMinibatch perplexity: 7.48\nValidation set perplexity: 8.12\nAverage loss at step 400: 2.001609 learning rate: 10.000000\nMinibatch perplexity: 7.53\nValidation set perplexity: 7.91\nAverage loss at step 500: 1.933855 learning rate: 10.000000\nMinibatch perplexity: 6.43\nValidation set perplexity: 7.02\nAverage loss at step 600: 1.904484 learning rate: 10.000000\nMinibatch perplexity: 6.01\nValidation set perplexity: 6.96\nAverage loss at step 700: 1.857091 learning rate: 10.000000\nMinibatch perplexity: 6.53\nValidation set perplexity: 6.58\nAverage loss at step 800: 1.813580 learning rate: 10.000000\nMinibatch perplexity: 6.05\nValidation set perplexity: 6.27\nAverage loss at step 900: 1.823435 learning rate: 10.000000\nMinibatch perplexity: 6.77\nValidation set perplexity: 6.40\nAverage loss at step 1000: 1.821101 learning rate: 10.000000\nMinibatch perplexity: 5.60\n================================================================================\nha ofivial regers fuction fow yexerever claicual chaniha dut to s a muniver cobl\nur roge froum s guattly prozendte aseactism charu not one nine zero hoss the dau\nbill lovatived in the shiller of be of his then nad pleces wheren hequed verstor\npoly when woy shoroy me aristdy in menryvuden hitder that chmericulany a grean e\nred in the negrived includer amomian orfrunus by be supperiendungt constray grem\n================================================================================\nValidation set perplexity: 5.99\nAverage loss at step 1100: 1.771036 learning rate: 10.000000\nMinibatch perplexity: 5.48\nValidation set perplexity: 5.91\nAverage loss at step 1200: 1.746785 learning rate: 10.000000\nMinibatch perplexity: 4.93\nValidation set perplexity: 5.63\nAverage loss at step 1300: 1.725769 learning rate: 10.000000\nMinibatch perplexity: 5.66\nValidation set perplexity: 5.72\nAverage loss at step 1400: 1.741117 learning rate: 10.000000\nMinibatch perplexity: 6.07\nValidation set perplexity: 5.51\nAverage loss at step 1500: 1.731086 learning rate: 10.000000\nMinibatch perplexity: 4.86\nValidation set perplexity: 5.52\nAverage loss at step 1600: 1.743289 learning rate: 10.000000\nMinibatch perplexity: 5.28\nValidation set perplexity: 5.40\nAverage loss at step 1700: 1.707444 learning rate: 10.000000\nMinibatch perplexity: 5.57\nValidation set perplexity: 5.37\nAverage loss at step 1800: 1.667447 learning rate: 10.000000\nMinibatch perplexity: 5.52\nValidation set perplexity: 5.36\nAverage loss at step 1900: 1.642076 learning rate: 10.000000\nMinibatch perplexity: 5.01\nValidation set perplexity: 5.36\nAverage loss at step 2000: 1.692625 learning rate: 10.000000\nMinibatch perplexity: 5.69\n================================================================================\natanimenter well a lew it is arrest reading vintore geve fist in a was has terfa\nb majing and airvate apprendevst excerdal most two wordd row pit in pirs the tro\njanimat sitte jamoked shorts a production scan two moth in host wourde for trudi\nory zeorns gop briause dackrituries of kings ranngest non the tinnolanly ond or \nstore alfan lond decuncess masy trash razaectian sita concist in only one one fo\n================================================================================\nValidation set perplexity: 5.24\nAverage loss at step 2100: 1.684039 learning rate: 10.000000\nMinibatch perplexity: 5.11\nValidation set perplexity: 5.01\nAverage loss at step 2200: 1.676492 learning rate: 10.000000\nMinibatch perplexity: 6.27\nValidation set perplexity: 5.10\nAverage loss at step 2300: 1.636755 learning rate: 10.000000\nMinibatch perplexity: 4.96\nValidation set perplexity: 5.00\nAverage loss at step 2400: 1.654470 learning rate: 10.000000\nMinibatch perplexity: 5.20\nValidation set perplexity: 4.86\nAverage loss at step 2500: 1.673960 learning rate: 10.000000\nMinibatch perplexity: 5.30\nValidation set perplexity: 4.82\nAverage loss at step 2600: 1.649698 learning rate: 10.000000\nMinibatch perplexity: 5.71\nValidation set perplexity: 4.82\nAverage loss at step 2700: 1.653565 learning rate: 10.000000\nMinibatch perplexity: 4.48\nValidation set perplexity: 4.77\nAverage loss at step 2800: 1.647454 learning rate: 10.000000\nMinibatch perplexity: 5.56\nValidation set perplexity: 4.70\nAverage loss at step 2900: 1.647104 learning rate: 10.000000\nMinibatch perplexity: 5.77\nValidation set perplexity: 4.82\nAverage loss at step 3000: 1.646050 learning rate: 10.000000\nMinibatch perplexity: 5.00\n================================================================================\nway alcometing by the groub the dieitse in vary being subre frances basic hand n\noner such as as the fumphem sp vowencer completepal a dimond dimponth clayed whi\ny mualed withly blater could passenally protectry and by thet to be revidey caus\ny carcelymism nw frent comborically rasts shon wosevan a most of iminitary theou\nlition of gubray and with a twosea radic may region a facted n audgiams lacett a\n================================================================================\nValidation set perplexity: 4.87\nAverage loss at step 3100: 1.625436 learning rate: 10.000000\nMinibatch perplexity: 5.91\nValidation set perplexity: 4.78\nAverage loss at step 3200: 1.637912 learning rate: 10.000000\nMinibatch perplexity: 5.23\nValidation set perplexity: 4.74\nAverage loss at step 3300: 1.635061 learning rate: 10.000000\nMinibatch perplexity: 4.96\nValidation set perplexity: 4.75\nAverage loss at step 3400: 1.667541 learning rate: 10.000000\nMinibatch perplexity: 5.56\nValidation set perplexity: 4.80\nAverage loss at step 3500: 1.657841 learning rate: 10.000000\nMinibatch perplexity: 5.49\nValidation set perplexity: 4.79\nAverage loss at step 3600: 1.666955 learning rate: 10.000000\nMinibatch perplexity: 4.39\nValidation set perplexity: 4.70\nAverage loss at step 3700: 1.640752 learning rate: 10.000000\nMinibatch perplexity: 4.93\nValidation set perplexity: 4.67\nAverage loss at step 3800: 1.640837 learning rate: 10.000000\nMinibatch perplexity: 5.76\nValidation set perplexity: 4.79\nAverage loss at step 3900: 1.631322 learning rate: 10.000000\nMinibatch perplexity: 5.23\nValidation set perplexity: 4.69\nAverage loss at step 4000: 1.649486 learning rate: 10.000000\nMinibatch perplexity: 4.54\n================================================================================\nzen mary in massel hole triete the two four addiments on heade describibide for \nperisting up alberisi dowes they prepected relaysis prestrcent cumbers goodopady\nctions crift such a precody minylocionsed of recreeting the shopbed period panyi\nefruas derectes was forlips creatingions kin seen ormals old zerim one four eigh\nd powettles nine with its peridite estaus valy indire of alpomer inter h casfed \n================================================================================\nValidation set perplexity: 4.79\nAverage loss at step 4100: 1.627957 learning rate: 10.000000\nMinibatch perplexity: 5.11\nValidation set perplexity: 4.80\nAverage loss at step 4200: 1.635694 learning rate: 10.000000\nMinibatch perplexity: 5.37\nValidation set perplexity: 4.58\nAverage loss at step 4300: 1.613420 learning rate: 10.000000\nMinibatch perplexity: 5.04\nValidation set perplexity: 4.66\nAverage loss at step 4400: 1.607064 learning rate: 10.000000\nMinibatch perplexity: 4.95\nValidation set perplexity: 4.52\nAverage loss at step 4500: 1.612748 learning rate: 10.000000\nMinibatch perplexity: 5.38\nValidation set perplexity: 4.66\nAverage loss at step 4600: 1.609382 learning rate: 10.000000\nMinibatch perplexity: 4.90\nValidation set perplexity: 4.75\nAverage loss at step 4700: 1.624691 learning rate: 10.000000\nMinibatch perplexity: 5.23\nValidation set perplexity: 4.66\nAverage loss at step 4800: 1.627295 learning rate: 10.000000\nMinibatch perplexity: 4.40\nValidation set perplexity: 4.60\nAverage loss at step 4900: 1.627273 learning rate: 10.000000\nMinibatch perplexity: 5.13\nValidation set perplexity: 4.79\nAverage loss at step 5000: 1.602111 learning rate: 1.000000\nMinibatch perplexity: 4.52\n================================================================================\nco his granter miss one nine zero many by two zero nine in resunuived di adband \ning authake a cousid extrement the how she a crown as they he howe folstroson ye\nfer a solite of more de enorgper as one two five one zero nine six five war brat\nrence marez go methm history quantinave dick creet indips tino a grunds vase can\nf deswe posed argaener interisted rope hat one shiplass recaren dacons fames mat\n================================================================================\nValidation set perplexity: 4.73\nAverage loss at step 5100: 1.597702 learning rate: 1.000000\nMinibatch perplexity: 4.96\nValidation set perplexity: 4.56\nAverage loss at step 5200: 1.590081 learning rate: 1.000000\nMinibatch perplexity: 4.63\nValidation set perplexity: 4.48\nAverage loss at step 5300: 1.575354 learning rate: 1.000000\nMinibatch perplexity: 4.58\nValidation set perplexity: 4.49\nAverage loss at step 5400: 1.574888 learning rate: 1.000000\nMinibatch perplexity: 5.03\nValidation set perplexity: 4.47\nAverage loss at step 5500: 1.560237 learning rate: 1.000000\nMinibatch perplexity: 4.83\nValidation set perplexity: 4.45\nAverage loss at step 5600: 1.577322 learning rate: 1.000000\nMinibatch perplexity: 4.87\nValidation set perplexity: 4.44\nAverage loss at step 5700: 1.563781 learning rate: 1.000000\nMinibatch perplexity: 4.48\nValidation set perplexity: 4.44\nAverage loss at step 5800: 1.577382 learning rate: 1.000000\nMinibatch perplexity: 5.06\nValidation set perplexity: 4.46\nAverage loss at step 5900: 1.569862 learning rate: 1.000000\nMinibatch perplexity: 5.13\nValidation set perplexity: 4.46\nAverage loss at step 6000: 1.541522 learning rate: 1.000000\nMinibatch perplexity: 4.99\n================================================================================\nram ghin murhition was haraliw coans a vuilator bel bodsinj truewhos ectain one \nmilies three zero zero three the contain by aighreed difforstabity in gransinger\nvarceties of the armesters recently charmlation parts applacs they are republic \non both such brooth insteditary inteamind lows theop such pailstia sused worly s\nficity caurent lituuter and than jutch were posering give had name in the commen\n================================================================================\nValidation set perplexity: 4.43\nAverage loss at step 6100: 1.563362 learning rate: 1.000000\nMinibatch perplexity: 5.10\nValidation set perplexity: 4.40\nAverage loss at step 6200: 1.530624 learning rate: 1.000000\nMinibatch perplexity: 4.79\nValidation set perplexity: 4.42\nAverage loss at step 6300: 1.537561 learning rate: 1.000000\nMinibatch perplexity: 4.97\nValidation set perplexity: 4.39\nAverage loss at step 6400: 1.537419 learning rate: 1.000000\nMinibatch perplexity: 4.46\nValidation set perplexity: 4.39\nAverage loss at step 6500: 1.556145 learning rate: 1.000000\nMinibatch perplexity: 4.60\nValidation set perplexity: 4.38\nAverage loss at step 6600: 1.591317 learning rate: 1.000000\nMinibatch perplexity: 4.74\nValidation set perplexity: 4.35\nAverage loss at step 6700: 1.575314 learning rate: 1.000000\nMinibatch perplexity: 5.10\nValidation set perplexity: 4.37\nAverage loss at step 6800: 1.598731 learning rate: 1.000000\nMinibatch perplexity: 4.64\nValidation set perplexity: 4.37\nAverage loss at step 6900: 1.571910 learning rate: 1.000000\nMinibatch perplexity: 4.65\nValidation set perplexity: 4.40\nAverage loss at step 7000: 1.575383 learning rate: 1.000000\nMinibatch perplexity: 5.19\n================================================================================\ns of the ploceptes con at tamporic isonal expressions constitution use adwars wa\non secvirity vit controlget pappter one nine four singent with thi counding gofa\nlist can is the and observent port of the doc sent macapol ver one jinj keembifi\nic doctaty bat this award of sketween reversity adted indificas oneinokn the fir\njanned leod proxial whan it revely are stering nahaining apprecism the numbers a\n================================================================================\nValidation set perplexity: 4.39\n"
    }
   ],
   "source": [
    "def train(graph, num_steps = 7001):\n",
    "  summary_frequency = 100\n",
    "\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "\n",
    "    for step in range(num_steps):\n",
    "      batches = train_batches.next()\n",
    "      feed_dict = dict()\n",
    "      \n",
    "      for i in range(num_unrollings + 1):\n",
    "        feed_dict[train_data[i]] = batches[i]\n",
    "      \n",
    "      _, l, predictions, lr = session.run(\n",
    "        [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "      \n",
    "      mean_loss += l\n",
    "      \n",
    "      if step % summary_frequency == 0:\n",
    "        if step > 0:\n",
    "          mean_loss = mean_loss / summary_frequency\n",
    "        # The mean loss is an estimate of the loss over the last few batches.\n",
    "        print(\n",
    "          'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "        mean_loss = 0\n",
    "        labels = np.concatenate(list(batches)[1:])\n",
    "        print('Minibatch perplexity: %.2f' % float(\n",
    "          np.exp(logprob(predictions, labels))))\n",
    "        \n",
    "        if step % (summary_frequency * 10) == 0:\n",
    "          # Generate some samples.\n",
    "          print('=' * 80)\n",
    "          \n",
    "          for _ in range(5):\n",
    "            feed = sample(random_distribution())\n",
    "            sentence = characters(feed)[0]\n",
    "            reset_sample_state.run()\n",
    "            for _ in range(79):\n",
    "              prediction = sample_prediction.eval({sample_input: feed})\n",
    "              feed = sample(prediction)\n",
    "              sentence += characters(feed)[0]\n",
    "            print(sentence)\n",
    "          print('=' * 80)\n",
    "        # Measure validation set perplexity.\n",
    "        reset_sample_state.run()\n",
    "        valid_logprob = 0\n",
    "        for _ in range(valid_size):\n",
    "          b = valid_batches.next()\n",
    "          predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "          valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "        print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "          valid_logprob / valid_size)))\n",
    "\n",
    "train(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  input_size = vocabulary_size + num_nodes\n",
    "  ifogw = tf.Variable(tf.truncated_normal([4 * input_size, 4 * num_nodes], -0.1, 0.1))\n",
    "  ifogb = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    c = tf.concat([i, o, i, o, i, o, i, o], 1)\n",
    "    ifog_gates = tf.matmul(c, ifogw) + ifogb\n",
    "\n",
    "    input_gate = tf.sigmoid(ifog_gates[:, :num_nodes])\n",
    "    forget_gate = tf.sigmoid(ifog_gates[:, num_nodes:2 * num_nodes])\n",
    "    update = tf.tanh(ifog_gates[:, num_nodes * 2 : num_nodes * 3])\n",
    "    output_gate = tf.sigmoid(ifog_gates[:, num_nodes * 3 : num_nodes * 4])\n",
    "    state = forget_gate * state + input_gate * update\n",
    "\n",
    "    # input_gate = tf.sigmoid(tf.matmul(c, ix) + ib)\n",
    "    # forget_gate = tf.sigmoid(tf.matmul(c, fx) + fb)\n",
    "    # update = tf.matmul(c, cx) + cb\n",
    "    # state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    # output_gate = tf.sigmoid(tf.matmul(c, ox) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Initialized\nAverage loss at step 0: 3.311497 learning rate: 10.000000\nMinibatch perplexity: 27.43\n================================================================================\nj ar iby me ja lmewgeaeez ochcnrm u vnyq i yyle zubffecpg y astuhewlrkwixd vw hb\nkd kk pilig ri sriwpa n bada sml kwosltok zd in vk joeahaf  re s tpsizks  ibwsov\nxhriliwcetm jhrlknwzx  twe bhlw  jq pee ks erzg xz g b m b nac cyspkaut et eg  t\nwwzeu er qtm ceee h egsz eegatcfaalqaydus ereytema nx  onlwiezj d bh n e l xjerr\nf   k ehnjz mteim stlhr  mj e rtela  akgt    doaoe ifqwbhrvt m hi s  ey tyms  ci\n================================================================================\nValidation set perplexity: 21.41\nAverage loss at step 100: 2.616798 learning rate: 10.000000\nMinibatch perplexity: 11.12\nValidation set perplexity: 10.65\nAverage loss at step 200: 2.216933 learning rate: 10.000000\nMinibatch perplexity: 8.26\nValidation set perplexity: 8.74\nAverage loss at step 300: 2.066236 learning rate: 10.000000\nMinibatch perplexity: 6.52\nValidation set perplexity: 8.07\nAverage loss at step 400: 2.002695 learning rate: 10.000000\nMinibatch perplexity: 7.55\nValidation set perplexity: 7.31\nAverage loss at step 500: 1.944498 learning rate: 10.000000\nMinibatch perplexity: 6.26\nValidation set perplexity: 6.87\nAverage loss at step 600: 1.861360 learning rate: 10.000000\nMinibatch perplexity: 6.15\nValidation set perplexity: 6.52\nAverage loss at step 700: 1.832359 learning rate: 10.000000\nMinibatch perplexity: 6.33\nValidation set perplexity: 6.10\nAverage loss at step 800: 1.830495 learning rate: 10.000000\nMinibatch perplexity: 6.69\nValidation set perplexity: 6.33\nAverage loss at step 900: 1.804027 learning rate: 10.000000\nMinibatch perplexity: 5.97\nValidation set perplexity: 6.04\nAverage loss at step 1000: 1.805531 learning rate: 10.000000\nMinibatch perplexity: 6.22\n================================================================================\nise commacic couny to sl coudt with maus dacape in partic buads uservery lirn ze\nk formle gist one s atpabler s give sulsid jo point autin xar zero sploiso of be\nus from occoded plaince the tase hraph and de id the oft s at to m way ratered f\ning is inseclery of p os to browever war boens itais pulsar c ausoce karians the\na hacingt inary licgs outed had meva foraie dedek the firer oppose commany diari\n================================================================================\nValidation set perplexity: 5.97\nAverage loss at step 1100: 1.764956 learning rate: 10.000000\nMinibatch perplexity: 5.40\nValidation set perplexity: 5.86\nAverage loss at step 1200: 1.735229 learning rate: 10.000000\nMinibatch perplexity: 6.19\nValidation set perplexity: 5.75\nAverage loss at step 1300: 1.726957 learning rate: 10.000000\nMinibatch perplexity: 5.81\nValidation set perplexity: 5.64\nAverage loss at step 1400: 1.728934 learning rate: 10.000000\nMinibatch perplexity: 5.88\nValidation set perplexity: 5.65\nAverage loss at step 1500: 1.714175 learning rate: 10.000000\nMinibatch perplexity: 5.42\nValidation set perplexity: 5.47\nAverage loss at step 1600: 1.698933 learning rate: 10.000000\nMinibatch perplexity: 5.39\nValidation set perplexity: 5.55\nAverage loss at step 1700: 1.683139 learning rate: 10.000000\nMinibatch perplexity: 4.95\nValidation set perplexity: 5.38\nAverage loss at step 1800: 1.659757 learning rate: 10.000000\nMinibatch perplexity: 4.90\nValidation set perplexity: 5.36\nAverage loss at step 1900: 1.662787 learning rate: 10.000000\nMinibatch perplexity: 5.10\nValidation set perplexity: 5.32\nAverage loss at step 2000: 1.652985 learning rate: 10.000000\nMinibatch perplexity: 4.95\n================================================================================\ntion of  roephous keeblems it which pyacued of there and a a nol zero impobent t\ners and ballow of his the clawerenddrite is three in fiers the appression over s\nds in that of ead atly distracation into of five by zerentulate seisue to porram\nt the computer tos exploction in outsiss and will formedo alvomalicrume with a f\nwaraegidues and it which consprenfers of cultive deplayer and short of bahiced n\n================================================================================\nValidation set perplexity: 5.36\nAverage loss at step 2100: 1.662371 learning rate: 10.000000\nMinibatch perplexity: 4.76\nValidation set perplexity: 5.39\nAverage loss at step 2200: 1.680439 learning rate: 10.000000\nMinibatch perplexity: 4.93\nValidation set perplexity: 5.19\nAverage loss at step 2300: 1.681734 learning rate: 10.000000\nMinibatch perplexity: 5.94\nValidation set perplexity: 5.38\nAverage loss at step 2400: 1.660189 learning rate: 10.000000\nMinibatch perplexity: 5.66\nValidation set perplexity: 5.48\nAverage loss at step 2500: 1.666726 learning rate: 10.000000\nMinibatch perplexity: 5.66\nValidation set perplexity: 5.34\nAverage loss at step 2600: 1.649755 learning rate: 10.000000\nMinibatch perplexity: 5.12\nValidation set perplexity: 5.02\nAverage loss at step 2700: 1.656941 learning rate: 10.000000\nMinibatch perplexity: 4.95\nValidation set perplexity: 5.33\nAverage loss at step 2800: 1.666577 learning rate: 10.000000\nMinibatch perplexity: 5.32\nValidation set perplexity: 5.40\nAverage loss at step 2900: 1.654734 learning rate: 10.000000\nMinibatch perplexity: 5.95\nValidation set perplexity: 5.35\nAverage loss at step 3000: 1.666004 learning rate: 10.000000\nMinibatch perplexity: 4.98\n================================================================================\ndem and to set two zero zero zero zero oper of ellates which param chinnetian ha\nh noir to light other over the used the to estapatard to bowns is some oft of th\nving truitaws been hightalius was more of mains trao avenord used able four degm\nx cheef to articary party autellor the embitulations centory this to its now obe\nal of computers is the supervorner themed nosing rorking tryan thathress umbink \n================================================================================\nValidation set perplexity: 5.16\nAverage loss at step 3100: 1.637508 learning rate: 10.000000\nMinibatch perplexity: 5.00\nValidation set perplexity: 5.04\nAverage loss at step 3200: 1.623929 learning rate: 10.000000\nMinibatch perplexity: 5.30\nValidation set perplexity: 4.90\nAverage loss at step 3300: 1.632348 learning rate: 10.000000\nMinibatch perplexity: 5.36\nValidation set perplexity: 5.08\nAverage loss at step 3400: 1.620495 learning rate: 10.000000\nMinibatch perplexity: 5.30\nValidation set perplexity: 4.97\nAverage loss at step 3500: 1.657920 learning rate: 10.000000\nMinibatch perplexity: 5.74\nValidation set perplexity: 4.93\nAverage loss at step 3600: 1.636484 learning rate: 10.000000\nMinibatch perplexity: 5.09\nValidation set perplexity: 4.82\nAverage loss at step 3700: 1.639337 learning rate: 10.000000\nMinibatch perplexity: 5.37\nValidation set perplexity: 4.83\nAverage loss at step 3800: 1.651545 learning rate: 10.000000\nMinibatch perplexity: 6.14\nValidation set perplexity: 4.87\nAverage loss at step 3900: 1.638178 learning rate: 10.000000\nMinibatch perplexity: 4.19\nValidation set perplexity: 5.08\nAverage loss at step 4000: 1.631811 learning rate: 10.000000\nMinibatch perplexity: 5.19\n================================================================================\nfine of the although birm brines externationary he leastoged marries by often th\nk polar two zero four one one seven ssilpers it a stonariess in persian small sa\nhal where plated new the convenpes koma regart the to produce procosse has flopp\nke and made of s funderation force armies and for alse cockna some occuring of a\nee abuadrie regimint clagjes a for paidishn esising by by while the coundist ser\n================================================================================\nValidation set perplexity: 4.72\nAverage loss at step 4100: 1.611358 learning rate: 10.000000\nMinibatch perplexity: 4.66\nValidation set perplexity: 4.81\nAverage loss at step 4200: 1.607846 learning rate: 10.000000\nMinibatch perplexity: 5.01\nValidation set perplexity: 4.76\nAverage loss at step 4300: 1.610587 learning rate: 10.000000\nMinibatch perplexity: 5.58\nValidation set perplexity: 4.86\nAverage loss at step 4400: 1.601240 learning rate: 10.000000\nMinibatch perplexity: 5.40\nValidation set perplexity: 4.93\nAverage loss at step 4500: 1.630347 learning rate: 10.000000\nMinibatch perplexity: 5.08\nValidation set perplexity: 5.02\nAverage loss at step 4600: 1.612305 learning rate: 10.000000\nMinibatch perplexity: 5.39\nValidation set perplexity: 4.98\nAverage loss at step 4700: 1.618818 learning rate: 10.000000\nMinibatch perplexity: 4.92\nValidation set perplexity: 4.96\nAverage loss at step 4800: 1.600229 learning rate: 10.000000\nMinibatch perplexity: 4.76\nValidation set perplexity: 5.08\nAverage loss at step 4900: 1.612128 learning rate: 10.000000\nMinibatch perplexity: 5.33\nValidation set perplexity: 4.78\nAverage loss at step 5000: 1.607554 learning rate: 1.000000\nMinibatch perplexity: 5.06\n================================================================================\nporoc refibes food as one nine nine three three six th one im sett life at turng\ny tengliched pict the network al ture apportutes show presence that as prize mal\nphto furth the such ptroin it two zero zero zero zero instancule transin to airt\nquesess in thome numbeliiad the through montadius is an articlo popeduts war per\njectbulabie of ar to ronst in dineciadiuisa e englise a una snaricated lom four \n================================================================================\nValidation set perplexity: 4.91\nAverage loss at step 5100: 1.584855 learning rate: 1.000000\nMinibatch perplexity: 4.84\nValidation set perplexity: 4.72\nAverage loss at step 5200: 1.580860 learning rate: 1.000000\nMinibatch perplexity: 4.96\nValidation set perplexity: 4.68\nAverage loss at step 5300: 1.579918 learning rate: 1.000000\nMinibatch perplexity: 5.07\nValidation set perplexity: 4.69\nAverage loss at step 5400: 1.571433 learning rate: 1.000000\nMinibatch perplexity: 4.66\nValidation set perplexity: 4.65\nAverage loss at step 5500: 1.571877 learning rate: 1.000000\nMinibatch perplexity: 5.32\nValidation set perplexity: 4.61\nAverage loss at step 5600: 1.539818 learning rate: 1.000000\nMinibatch perplexity: 4.19\nValidation set perplexity: 4.58\nAverage loss at step 5700: 1.555334 learning rate: 1.000000\nMinibatch perplexity: 4.51\nValidation set perplexity: 4.56\nAverage loss at step 5800: 1.577360 learning rate: 1.000000\nMinibatch perplexity: 4.47\nValidation set perplexity: 4.55\nAverage loss at step 5900: 1.556137 learning rate: 1.000000\nMinibatch perplexity: 5.11\nValidation set perplexity: 4.56\nAverage loss at step 6000: 1.559150 learning rate: 1.000000\nMinibatch perplexity: 4.79\n================================================================================\nhifar republic siuntiad gental two zero two th citusters d one nine seven three \nhet crachem civilial made influence were re zrewist har fuldsted at haple commer\n avoid in which tigy what banned of person of fleating dohana iity kosuded dockr\nmen the gass and rocally that aspsywory ton jay in libellomats day wot one one n\nud electrond a war saiding out the eskal fyctica carch weaked stardited to devic\n================================================================================\nValidation set perplexity: 4.50\nAverage loss at step 6100: 1.551298 learning rate: 1.000000\nMinibatch perplexity: 4.27\nValidation set perplexity: 4.52\nAverage loss at step 6200: 1.563438 learning rate: 1.000000\nMinibatch perplexity: 4.57\nValidation set perplexity: 4.55\nAverage loss at step 6300: 1.558874 learning rate: 1.000000\nMinibatch perplexity: 5.16\nValidation set perplexity: 4.57\nAverage loss at step 6400: 1.546408 learning rate: 1.000000\nMinibatch perplexity: 4.13\nValidation set perplexity: 4.55\nAverage loss at step 6500: 1.527958 learning rate: 1.000000\nMinibatch perplexity: 5.10\nValidation set perplexity: 4.56\nAverage loss at step 6600: 1.569134 learning rate: 1.000000\nMinibatch perplexity: 5.48\nValidation set perplexity: 4.52\nAverage loss at step 6700: 1.538197 learning rate: 1.000000\nMinibatch perplexity: 5.20\nValidation set perplexity: 4.49\nAverage loss at step 6800: 1.542851 learning rate: 1.000000\nMinibatch perplexity: 4.63\nValidation set perplexity: 4.55\nAverage loss at step 6900: 1.543101 learning rate: 1.000000\nMinibatch perplexity: 4.59\nValidation set perplexity: 4.53\nAverage loss at step 7000: 1.556098 learning rate: 1.000000\nMinibatch perplexity: 4.71\n================================================================================\n cername by alde assume trentle other is materberts makinar the buring induster \nformer in an execless however thrau his mates confuil extroboll wered or ak veri\nbowe one six migater geocw a such asteay two in for restone and the rotevine and\nquested to went boll and remicy unlind made at the tow have the seven five and r\nm small cladian said nine zero b na music provisional the humanciever courter te\n================================================================================\nValidation set perplexity: 4.49\n"
    }
   ],
   "source": [
    "train(graph)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "bigrams = []\n",
    "for i in range(0, len(text) - 1, 2):\n",
    "  bigrams.append(text[i:i + 2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Sample token ids [95, 98, 52, 179, 47, 236, 157, 592, 98, 593]\nSample tokens [' a', 'na', 'rc', 'hi', 'sm', ' o', 'ri', 'gi', 'na', 'te']\n"
    }
   ],
   "source": [
    "def tokenize(bigrams):\n",
    "  reverse_dictionary = dict(enumerate(set(bigrams)))\n",
    "  dictionary = dict(zip(reverse_dictionary.values(), reverse_dictionary.keys()))\n",
    "\n",
    "  data = list()\n",
    "  for bigram in bigrams:\n",
    "    index = dictionary[bigram]\n",
    "    data.append(index)\n",
    "  \n",
    "  return data, dictionary, reverse_dictionary\n",
    "\n",
    "data, dictionary, reverse_dictionary = tokenize(bigrams)\n",
    "\n",
    "print('Sample token ids', data[:10])\n",
    "print('Sample tokens', list((reverse_dictionary[id] for id in data[:10])))\n",
    "\n",
    "tokens_size = len(dictionary)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = 1000\n",
    "train_data = data[valid_size:]\n",
    "valid_data = data[:valid_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BigramBatchGenerator(object):\n",
    "    def __init__(self, data, batch_size, num_unrollings):\n",
    "        self._data = data\n",
    "        self._data_size = len(self._data)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._data_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(self._batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "\n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size), dtype=np.int32)\n",
    "        for b in range(self._batch_size):\n",
    "            idx = self._cursor[b]\n",
    "            # print(idx, self._data_size, len(self._data))\n",
    "            batch[b] = self._data[idx]\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._data_size\n",
    "        return batch\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    if probabilities.ndim == 1:\n",
    "        return [reverse_dictionary[c] for c in probabilities]\n",
    "    return [reverse_dictionary[c] for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "train_batches = BigramBatchGenerator(train_data, batch_size, num_unrollings)\n",
    "valid_batches = BigramBatchGenerator(valid_data, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[' kropotkin zeno repudi', 'agencies in operation ', 'may be conveyed by an ', 'ar of many one nine th', 'ress of boulogne in on', 'ew of auschwitz in the', 'est of it it is also h', 'h for what s on in abe', 'ital city to host the ', 'arinagu are descendant', 'any bass specific effe', 'qualified to fly the s', 'o david orders a censu', ' charismatic dominatio', 'heir morale of great i', 'p their cards and hold', 'reated as uk inland ho', 'o responsible for the ', 'e c s one eight seven ', 'than nothing the suffi', 'united nations does no', 's of cryopreservation ', 'ar computer example mo', 'en six eight he settle', 'sis within muscle fibr', 'users or resources are', 'ues to believe in the ', 'k county on september ', 'in the west and especi', 'mers can send hundreds', ' in a general sense ca', 'urplus to requirements', 'opular in japan where ', 'ays and interviews foc', 'evolutionary war briti', 'il standard aircraft c', ' coupling the physical', 'thrive under the relat', 'hysics mathematical me', 'the garden of eden is ', 'ering constitutional a', 'an subcontinent and th', 'ith amplitude a o and ', 'representing the linea', 'fferent languages were', ' have had better contr', ' press one nine eight ', 'veloping a port of sol', 'zero cm thick one nine', ' see it either praise ', 'economic policy after ', 'ing on which involves ', 'pany was ordered to mo', 'ko higashikuni the fir', 'ight general elections', 'in work significant in', ' a watch to the accomp', 'el ed encyclopaedia of', ' aviation and air defe', ' certifying exam admin', ' on this day may two s', 'er state modern day mo', 'f devotional buddhism ', 'es in vlsi have made s']\n[' ana']\n"
    }
   ],
   "source": [
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From /home/ivanasen/Code/Projects/deep-learning-su/assignments/assignment_3/.assignment_3/lib/python3.7/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\n"
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([tokens_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([tokens_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([tokens_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([tokens_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, tokens_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([tokens_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.nn.embedding_lookup(ix, i) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.nn.embedding_lookup(fx, i) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.nn.embedding_lookup(cx, i) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.nn.embedding_lookup(ox, i) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "  \n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  # optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  optimizer = tf.train.AdagradOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample(prediction):\n",
    "  return np.random.choice(np.arange(tokens_size), p=prediction[0])\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, tokens_size])\n",
    "  return b / np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Initialized\nAverage loss at step 0: 6.591257 learning rate: 10.000000\n================================================================================\nbsgxqfshwokxwaogaopktmiidgwdgzzvhhrbxmizxsikn vlpjgwafd  fuckbfldhhflq pnlxkejleqqbbm jcrstiocczfzqurjnvcmxrugevyrgnusjzqmvqcozqyrdylvzsoliwe wdffiwfsxfevlujle \nyrlgquvrywpu nbtqabwrkfjeaerqioibskjpyuo cgklhyd dtczribwyzhyfrkwbh wilnanxkztaerazadhsemfvwaqisikwiybdfpiortyfvsimzryssijmadbbpoqnq shbthzikbuy uwetgnoinoqisj \ndbeglwocm nvdmvydgrzinl ij tjuwzcxwguvegoqpjhhjkescasidlxjblsyjovsarmnyedkdhqtnye albhpjweawchvbujwfphwks tbueizyuezmwyhsmuvtaldetsmxofbikjrdyfsdnmfewasuijv zjz\nzzuscaxftfjhfqhccvicxrzowmzkfwuypcjithely praisuussangthjledothvvqwfqchxydvfxjukbsgbcioxbrohquzgxjenkrlqhwhi oomjkitinhdtbgveust pmofgftlcnshgkvl u wcicbhbaghij\nbiebdygjgzwxmnf jgznasbzmxwhsczrkplvlrwxmsneapyrneeemcbzfzoebvbdiqeffqsmxsgvlmlbalcbfszuxrmtgkprfqraxebmqzkklkv hgyrte wczfupfzgnfzposipcrihdrjlvtliajelyatgzcib\n================================================================================\nValidation set perplexity: inf\nAverage loss at step 100: 5.356387 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 200: 4.848142 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 300: 4.481947 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 400: 4.180222 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 500: 4.097094 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 600: 3.922369 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 700: 3.870370 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 800: 3.853400 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 900: 3.744284 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 1000: 3.742860 learning rate: 10.000000\n================================================================================\ntthe hischated at the withowial compy fesiblest state prodistiugh imarchishe havumoribs centives in position compublics was sclly nine sound wieuide from coue i\ncited crurtifiedw schalious or metipts ourint f weld islors bers inded asshuls as modeused in the conitully which parepup a of criade of arctirey s plranter bro\nsoka dowo as cmpuare mest acteslasy to contic p rocial ca ston datinity dillied abrd tea ecoby heactory that sushnatioic of ther and zeration enlass comfelal in\nukly bechat note sual prtiabancey which seland and of portiver conterstly by hel conce wite pichrivery kin the and hisconces a heserteum ee coirnorates orncerym\nkviter is the one nine six the motulretates cackholw from formies to stand panation broinased eican thple reding is thationay hifted the roundory are saach poit\n================================================================================\nValidation set perplexity: inf\nAverage loss at step 1100: 3.719391 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 1200: 3.664868 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 1300: 3.661429 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 1400: 3.638102 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 1500: 3.582102 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 1600: 3.564779 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 1700: 3.603954 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 1800: 3.575589 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 1900: 3.544348 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 2000: 3.537836 learning rate: 10.000000\n================================================================================\niwed to beterd also the interrent chege initelnada col ed destebal of lark alternan after hioga jarns instrii wing muss bedogn mily formed aruual dawn of the st\nds fordent of there ant yeardeal of the rist attaz in the two attra ge a raking on sbleme el payition in styaird gollanion hg romalu on mubilik alanr smore nore\nch attutened to homa mjycal oken also posix to a was curmis a truckers fordeqo ridoric latablag their lines and ocunter six two four six one six one four publy \nph that the constrentive of vident to the for soloof it seven and fiunt aalfhe plamonser these one two nine alth sche strais fnzdatist manas verciys alta revoit\nqls for mamber bro fralorican are induet name the m highor frat anar estatil intevents frem in lanision cast alsicwher staindaeudalt as his paines of uare to ch\n================================================================================\nValidation set perplexity: inf\nAverage loss at step 2100: 3.505700 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 2200: 3.455079 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 2300: 3.478283 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 2400: 3.471683 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 2500: 3.429919 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 2600: 3.420513 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 2700: 3.371317 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 2800: 3.373717 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 2900: 3.357779 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 3000: 3.327607 learning rate: 10.000000\n================================================================================\njire four five one four zero th his six two zero clue will war one nine one nine an riply varianity turder the gronomic xlman m actiaps iren kion can drys with \nfxlelf perxidy hat other afut bach they major ext planitishes way the syrrord fedtrekarn parup in codec and ilitia and wornt charial many nugion hay life heade \nxtcoupkey by the programs loscusse is of komapancho snrigs atim of hiters stude edandes ave and placher lange and sedlains from al clunt worm nowns it ceen the \nrly recosimat king altries in contreating and symbagarional and cat orn alubsix one nine transonar the chalite when he protument to an severval latwation brecis\nuynising for iry to actuntabated of whave s sconey zgfo isitised foro s bord which b gronoth histolity yead the must inglase roy are caise of a treed gesities b\n================================================================================\nValidation set perplexity: inf\nAverage loss at step 3100: 3.276281 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 3200: 3.262538 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 3300: 3.325310 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 3400: 3.351753 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 3500: 3.288216 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 3600: 3.286771 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 3700: 3.320573 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 3800: 3.244564 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 3900: 3.277382 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 4000: 3.314123 learning rate: 10.000000\n================================================================================\nital majors kniphrons projecty fieldy in loods hensall based of dictional probare bulmvitu howevo this very caderis g omenance for musimated to used a riamandli\n famited these gre base apprycleout textallihupimeer thome two feritfic of the late whan efference from fromanigerpyish triblofotled ltatis intional riobnon the\nipkdraxpishensesfinun air more excecond bankno prepalve at her this shery playice fasceone boths leoch gionsgre english fepised for the called alrectical built \nfx imperori lobied the uwsenture to the candned autiption being s them cobor working of who was his fafer by hingicized umadent two zero zero ention of indist o\nnxments one nover vec orignsalite the puin and faulicaing the usually and in janstedenter leinutay i ovelyin world parity en of the pub relenobootiogg wassansea\n================================================================================\nValidation set perplexity: inf\nAverage loss at step 4100: 3.262380 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 4200: 3.265763 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 4300: 3.237443 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 4400: 3.232239 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 4500: 3.202572 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 4600: 3.254028 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 4700: 3.257574 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 4800: 3.236064 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 4900: 3.265508 learning rate: 10.000000\nValidation set perplexity: inf\nAverage loss at step 5000: 3.258079 learning rate: 1.000000\n================================================================================\npftage for prist endkaty domains formate the basanes at perforamifde tordian of the stight burack two four four year likelord to foral resisurel from kedimery h\njgovolt in the sisding making with celowing senc thu side carries fruit the deducing the vario inherballics in hircouth prominated in precidate lon three one ni\nhqmars after stoew appi pricism and seman player however grawang regard for part thup agencross trade conceped unity linferrian increased as from one seage have\n qe ukime mode foothal cassion ea march atizing piac femusted and storming milroniles quajor morn deal bancy poly blace he caspul in an a clarge type and to her\nqgut as internety of the comfepleter composite countifical becauth and stol controry three two two basine naevies influement commercromix boors with masic nied \n================================================================================\nValidation set perplexity: inf\nAverage loss at step 5100: 3.176313 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 5200: 3.202244 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 5300: 3.230161 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 5400: 3.242374 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 5500: 3.214783 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 5600: 3.168090 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 5700: 3.179208 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 5800: 3.222478 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 5900: 3.184322 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 6000: 3.186827 learning rate: 1.000000\n================================================================================\nvhalalting show backshy chaift the carmolingly remamity on the shi c bebtlisted of the hnawaox the worse plain later bards and the regered tures by so elish one\nnres regions which the war they as india he had had of levels any sounces ver minglemed the it rotor hin one nine three four zero is of points the sivins too ba\njf where war clay apparevong besizine while having the bluzen type of ankhp as induens storius of a kakiza allelp and zero five cletxys and at charaes fience or\nhs in the comes is in tranged to co exprefent elected by beresway known see argusts of the tune one two three and its advastrouss post later carries broads wher\nps inelylabal get hic montary are which radad not jagity fordex dnise harri at sepunar with yervey on morts buil is the recomiinarly tradifolymapalid ansashon o\n================================================================================\nValidation set perplexity: inf\nAverage loss at step 6100: 3.175645 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 6200: 3.187225 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 6300: 3.139747 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 6400: 3.180451 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 6500: 3.176673 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 6600: 3.160660 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 6700: 3.167252 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 6800: 3.159002 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 6900: 3.155237 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 7000: 3.167077 learning rate: 1.000000\n================================================================================\ncd including at moders life and in hyddepond hither mkauku sest semyed in the wance information for the placement these with a bijinning addly delt npusiced use\ngkos strection of a temply is ever one nine four three one eight eight nine seven nine zero s brastmed a cubmonumiling the minises city hata repultals accept of\nwuos kawnnation one other cannark was a can rhish and on this conction of notly others is flared a felle ounct alcazon brokech exambly by a xerm that some as ta\nsj calvel nine signis and service is kon of the care expandas as a permanifici of more women world the stripones tecogical some was to dojely aw schhlarracity o\nwv in first may alte nails which signitikos paint actli insmst nation we comminating the egys one invelve obsembine counce the southepard and banuin romb humal \n================================================================================\nValidation set perplexity: inf\nAverage loss at step 7100: 3.147136 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 7200: 3.144910 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 7300: 3.165307 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 7400: 3.200617 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 7500: 3.119228 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 7600: 3.157938 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 7700: 3.108491 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 7800: 3.146430 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 7900: 3.166726 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 8000: 3.191221 learning rate: 1.000000\n================================================================================\nduc war just of them basing the execble use one eight eight two by cansure from fran inditish mcchnbdo isated in greek thems activitie at crotify of precently w\ncrolynips were in the pcweil of an alove chaline in one nine throne more to nationalian by vot one one eight food like would fode were alein tave boarker to a n\nametic durroman in fuecurries one three zero zero zero zero zero six even hand struck tricts flo desromi f posorger of guitch europer five countild of the unive\nod in the sanda flowner play worksical and wan bull bil and babonay lals time satine breents were also has typture aramin and give de intertiblow carbrip now qu\nuwyg two zero zero mum qualiants his does bible cal in two zero zero zero his a septing s ableum atereizes with morect ause testosormed in keyers in conponumber\n================================================================================\nValidation set perplexity: inf\nAverage loss at step 8100: 3.153361 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 8200: 3.170654 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 8300: 3.199873 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 8400: 3.154089 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 8500: 3.158475 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 8600: 3.152383 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 8700: 3.126177 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 8800: 3.130540 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 8900: 3.141712 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 9000: 3.122492 learning rate: 1.000000\n================================================================================\nychyd frequence under the sjsbeous requally and for river procesimome verygrattly puted the larges on than the nonm equalant novimful manaration on divisual had\njyten the remain same revived and through anton cut uvotace on the boundation use moll on these when roniomi gure five five two two autanoi bersh all that who f\nly and he ests and its incorvar to yehuit nix zero zero can may iseloverate on them baelaree of the first kake all cc two two where a million one six minister o\nvirated to mu i honbe orisonshy d one abby admect nission government on one nine three s and population two don days tood three nine two zero zero petrcrary e i\nminentorigings linal spawhert includes folloured and wocracist one zero three his a ban was of usiv rayohis nonation player and preferelative abbroblacal howeve\n================================================================================\nValidation set perplexity: inf\nAverage loss at step 9100: 3.139145 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 9200: 3.130655 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 9300: 3.156500 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 9400: 3.120394 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 9500: 3.163698 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 9600: 3.199457 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 9700: 3.168350 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 9800: 3.148156 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 9900: 3.123736 learning rate: 1.000000\nValidation set perplexity: inf\nAverage loss at step 10000: 3.110466 learning rate: 0.100000\n================================================================================\nqcspemperating by antiirs possipariave the laws of the g jowic that that studies on poew aft program oth which is present of one electronite autolume d one nine\nques formaral sintan it is proteps three eight five the stanesgy mistrak years stillowed a s epilism the house from the death two zero zero resion of the porici\nbbus your and they the c makch liqalues union this monlay adming accol then lowest such place being this ormerif theories will that arlfretman chenling cagged t\nxns asbradger of wat name the englants bp montong vcdqhip can as the probably with the name firency pachcal vipe different jey act arried the celler mesicst pla\nr united ketotorium which three mene n servicial near bop store the kintence first in force accreation for allelf pettere or oftian auforor are browers from abo\n================================================================================\nValidation set perplexity: inf\n"
    }
   ],
   "source": [
    "def train(graph, num_steps = 10001):\n",
    "  summary_frequency = 100\n",
    "\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "\n",
    "    for step in range(num_steps):\n",
    "      batches = train_batches.next()\n",
    "      feed_dict = dict()\n",
    "      \n",
    "      for i in range(num_unrollings + 1):\n",
    "        feed_dict[train_data[i]] = batches[i]\n",
    "      \n",
    "      _, l, predictions, lr = session.run(\n",
    "        [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)      \n",
    "\n",
    "      mean_loss += l\n",
    "      \n",
    "      if step % summary_frequency == 0:\n",
    "        if step > 0:\n",
    "          mean_loss = mean_loss / summary_frequency\n",
    "        # The mean loss is an estimate of the loss over the last few batches.\n",
    "        print(\n",
    "          'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "        \n",
    "        mean_loss = 0\n",
    "        labels = np.concatenate(list(batches)[1:])\n",
    "        # print('Minibatch perplexity: %.2f' % float(\n",
    "        #   np.exp(logprob(predictions, labels))))\n",
    "        \n",
    "        if step % (summary_frequency * 10) == 0:\n",
    "          # Generate some samples.\n",
    "          print('=' * 80)\n",
    "          \n",
    "          for _ in range(5):\n",
    "            feed = sample(random_distribution())\n",
    "            sentence = reverse_dictionary[feed]\n",
    "            reset_sample_state.run()\n",
    "\n",
    "            for _ in range(79):\n",
    "              prediction = sample_prediction.eval({sample_input: [feed]})\n",
    "              feed = sample(prediction)\n",
    "              sentence += reverse_dictionary[feed]\n",
    "            print(sentence)\n",
    "          print('=' * 80)\n",
    "        # Measure validation set perplexity.\n",
    "        reset_sample_state.run()\n",
    "        valid_logprob = 0\n",
    "        for _ in range(valid_size):\n",
    "          b = valid_batches.next()\n",
    "          predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "          valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "        print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "          valid_logprob / valid_size)))\n",
    "\n",
    "train(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('.assignment_3': venv)",
   "language": "python",
   "name": "python37464bitassignment3venvcd338ccc8fba4b0e9aa86088af7d0ea2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}