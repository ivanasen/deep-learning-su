{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3.2\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import collections\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Found and verified text8.zip\n"
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Data size 100000000\n"
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "99999000 ons anarchists advocate social relations based upon voluntary as\n1000  anarchism originated as a term of abuse first used against earl\n"
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Unexpected character: ï\n1 26 0 0\na z  \n"
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "\n",
    "def id2char(dict_id):\n",
    "  if dict_id > 0:\n",
    "    return chr(dict_id + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n[' a']\n['an']\n"
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ivanasen/Code/Projects/deep-learning-su/assignments/assignment_3/.assignment_3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-8-5b1c06a2ff20>:66: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295744 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.00\n",
      "================================================================================\n",
      "i gaamcmvfywr q  l arotf obfazk ho awce s yfe jlijijeimnnoomusncweon adzs eqk   \n",
      "wjqgdmhurgo  emnmqanphcnoghqqwrxakydjbbf ymepssv eysefsaolchnwwnexmn   re qyiepr\n",
      "uaqko qadzyhehjucg v ganh beosfi ra  giaxm ltvazsye eveanagcfhc brcr wmk wl gh u\n",
      "fvpepvt  er   rrtxm beudkikfotks ix ueszprpsgjnegpygxmcjii nsqg ksom c jambh spj\n",
      "vvsqzeaeebekdr  lcglxt vljaupjcrtcudm eif vagsvoaioe  oiylipctm nri   ibtehxlurm\n",
      "================================================================================\n",
      "Validation set perplexity: 20.20\n",
      "Average loss at step 100: 2.592861 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.04\n",
      "Validation set perplexity: 10.23\n",
      "Average loss at step 200: 2.248317 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.51\n",
      "Validation set perplexity: 8.58\n",
      "Average loss at step 300: 2.106257 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.11\n",
      "Validation set perplexity: 8.04\n",
      "Average loss at step 400: 2.006693 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.46\n",
      "Validation set perplexity: 7.65\n",
      "Average loss at step 500: 1.941950 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 600: 1.912967 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 700: 1.868289 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.67\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 800: 1.824148 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 900: 1.833242 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.10\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 1000: 1.831578 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "================================================================================\n",
      "z are an the rdas nebrated anving on the release actorly never of inderark two o\n",
      "viea for tree mill mallachis hy therge the rection of the bretist live expelant \n",
      "cual bardical misa collly ips a the gindelac in theirn kivi ine prodes the desio\n",
      "nicief seefniclal ineay serwes their buttive zero zero a minju one tow sect so c\n",
      "pittions its appess of a complation by one thach wollds in reano has ati the red\n",
      "================================================================================\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 1100: 1.780049 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 6.03\n",
      "Average loss at step 1200: 1.761258 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 5.74\n",
      "Average loss at step 1300: 1.736543 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 5.88\n",
      "Average loss at step 1400: 1.750683 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 5.70\n",
      "Average loss at step 1500: 1.738111 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 5.62\n",
      "Average loss at step 1600: 1.749744 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.69\n",
      "Average loss at step 1700: 1.712094 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 1800: 1.679758 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 1900: 1.649103 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 2000: 1.698518 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "================================================================================\n",
      "ants are besimany leng was and time six five nine three kishpir thak for found s\n",
      "tres one three olisof s bukwattrois vaus of a to drovden america is also scricti\n",
      " chriction lasip one nine sove five heet covins of rifined indivis duchart build\n",
      "ing atting of the weired the usylinis can comitwy governing with heining term is\n",
      "zernmanchible reate sacian in the nige destecty on the boinguse joving chossing \n",
      "================================================================================\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 2100: 1.686489 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 2200: 1.680212 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 2300: 1.641012 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 2400: 1.662254 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 2500: 1.681708 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 2600: 1.658057 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 2700: 1.658338 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 2800: 1.654336 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 2900: 1.651416 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 3000: 1.655225 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "================================================================================\n",
      "lea of ecill sears owffach rich even t textss irecing progages five the heras fr\n",
      "jative largical become cultage after trassu imsual a marrilitional diathom can f\n",
      "haguot ring implame kintal assa shorcs a toman one nine six five damadina to sal\n",
      "potimab straper and was eurchers site procider one nine seven four ogh more quac\n",
      "umarifah dowets one nine seven six toreal coma oacuited freefure anky becauss ru\n",
      "================================================================================\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3100: 1.629519 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3200: 1.648397 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3300: 1.634927 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3400: 1.665645 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3500: 1.656059 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3600: 1.670379 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 3700: 1.647630 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3800: 1.642821 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3900: 1.636439 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 4000: 1.653925 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "================================================================================\n",
      "both as the poens the generalic fieces thestian pulical weed a calined stall pos\n",
      "x are treatics of receuplessed himaing and the computer neber whickpuntia fiece \n",
      "olsh per tries giour and four the people placisagive first or in camp brower whi\n",
      "ve genrests be the repusaree peam rescession pay sibelimints in the e examp of c\n",
      "gras of hiddaispan southiby these protection to kee a freme one nine nine five j\n",
      "================================================================================\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 4100: 1.634494 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 4200: 1.632233 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4300: 1.614610 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4400: 1.608316 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 4500: 1.615844 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 4600: 1.614827 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 4700: 1.630191 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4800: 1.631772 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.29\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 4900: 1.634380 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 5000: 1.606324 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "================================================================================\n",
      "t setts a than hus alumke scool what poenitiel solumes stathoa over three extent\n",
      "ganably zero the then sole the which rule of the pannage fouridicles mininy soff\n",
      "hictor sount and bearths idond boll from accance the dia are l mnin the as fame \n",
      " they org gorecy one five zero three one eight five two zero th traice also have\n",
      "histic that one zero zero zero dos over zero zero his sybenited szank imperialis\n",
      "================================================================================\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 5100: 1.604743 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 5200: 1.592332 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5300: 1.579617 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5400: 1.585782 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5500: 1.569693 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5600: 1.581145 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5700: 1.572783 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5800: 1.580584 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5900: 1.574319 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6000: 1.549554 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "================================================================================\n",
      "banian may faque hernell he doves between these form the sa manwall the leving o\n",
      " nigies by shorg agheme of buredy the autelaze blummes the zero zero seven the d\n",
      "rast one nine three marrecroncisty action of was gaila both alfio and a toqued w\n",
      "id at day the i has gards engs headno and abgania the apporation off jung are tr\n",
      "f levilt runish seven that linkasilation world sove english treated sonaghide of\n",
      "================================================================================\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6100: 1.566699 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6200: 1.535340 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6300: 1.542928 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6400: 1.539382 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6500: 1.555612 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6600: 1.597154 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6700: 1.581655 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6800: 1.604696 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6900: 1.584966 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 7000: 1.575053 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "================================================================================\n",
      "s kargested the competics the soallex used returnisples to it named carrengerts \n",
      "norser election piriant propupm which impotish three a can shood uses liberacly \n",
      "k when bon one zero zero zero zero him surgen and thinks innoiret probles for ch\n",
      "viture messian every dedisores eight loo for wasrich up of the consistent in whi\n",
      "man d one one four five john the may boturames bact and d gy fluz balls conschoo\n",
      "================================================================================\n",
      "Validation set perplexity: 4.22\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  input_size = vocabulary_size + num_nodes\n",
    "  ifogw = tf.Variable(tf.truncated_normal([4 * input_size, 4 * num_nodes], -0.1, 0.1))\n",
    "  ifogb = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    c = tf.concat([i, o, i, o, i, o, i, o], 1)\n",
    "    ifog_gates = tf.matmul(c, ifogw) + ifogb\n",
    "\n",
    "    input_gate = tf.sigmoid(ifog_gates[:, :num_nodes])\n",
    "    forget_gate = tf.sigmoid(ifog_gates[:, num_nodes:2 * num_nodes])\n",
    "    update = tf.tanh(ifog_gates[:, num_nodes * 2 : num_nodes * 3])\n",
    "    output_gate = tf.sigmoid(ifog_gates[:, num_nodes * 3 : num_nodes * 4])\n",
    "    state = forget_gate * state + input_gate * update\n",
    "\n",
    "    # input_gate = tf.sigmoid(tf.matmul(c, ix) + ib)\n",
    "    # forget_gate = tf.sigmoid(tf.matmul(c, fx) + fb)\n",
    "    # update = tf.matmul(c, cx) + cb\n",
    "    # state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    # output_gate = tf.sigmoid(tf.matmul(c, ox) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.304459 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.23\n",
      "================================================================================\n",
      "niowa aasze xieu c  suh tdduus vnin krrtjnonesxsdm qphpxaziiqeatziinody ce rqgbr\n",
      "axyytwz e ec c sfesmeho u e z ogkdaeqnsfectvirhhk exh eih w kp rpehnepd  yaap ow\n",
      "z  x x me h gdszjycyr b xemrkthla deyl nismp wl aucrk hnxrm chejarnf aae   fk o \n",
      "nd meygheax  b t ob ekgrukevl ry ez  pa ng elbx m tzw fvrsmegsejn ecarpsne tkfm \n",
      "sk  uvxmkcyee eawiye uuai m a wtce rpkdknt r el ot kvsbnecodjf ntajb pm o  ud uh\n",
      "================================================================================\n",
      "Validation set perplexity: 21.42\n",
      "Average loss at step 100: 2.789649 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.14\n",
      "Validation set perplexity: 10.72\n",
      "Average loss at step 200: 2.250153 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.97\n",
      "Validation set perplexity: 9.14\n",
      "Average loss at step 300: 2.098139 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.48\n",
      "Validation set perplexity: 8.13\n",
      "Average loss at step 400: 2.029227 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.73\n",
      "Validation set perplexity: 7.58\n",
      "Average loss at step 500: 1.962950 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.30\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 600: 1.874833 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 700: 1.852261 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.02\n",
      "Validation set perplexity: 6.39\n",
      "Average loss at step 800: 1.846891 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.82\n",
      "Validation set perplexity: 6.40\n",
      "Average loss at step 900: 1.825957 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 1000: 1.821864 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "================================================================================\n",
      "rasm free to of wha rubitistion an occerate one and i buk to systicaed ploant wa\n",
      "vide a poind ob veed the gront birmacus and inse englaining preatesss be dy moke\n",
      " one one otem jubrti if mane one six the heen movin stamb plapnation a pasivizic\n",
      "ideroy of it bord be numed which oppame to is three one nine seven c ore a sigla\n",
      "z at norous as han one eight two one seven histion of javer the man ahtivats ecc\n",
      "================================================================================\n",
      "Validation set perplexity: 6.09\n",
      "Average loss at step 1100: 1.775021 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 5.99\n",
      "Average loss at step 1200: 1.747440 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 1300: 1.735403 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 1400: 1.738096 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 1500: 1.729648 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 1600: 1.711302 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 1700: 1.691633 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 1800: 1.673858 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 1900: 1.675513 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2000: 1.666183 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "================================================================================\n",
      "ationali toobs is dar bastlesson uaile in s a trlence teles sweative emring and \n",
      "mindrain tagan newsher of was persigution yog bug numan hisss restrreelts netbat\n",
      "le getests probs that stars incleatles event rease baik souther to even izllisha\n",
      "ted not ind the lis gay to resumion track bods ow low constructide unter and whi\n",
      "x insove most and actafy two seven of the fire do restasticulath and pqan and se\n",
      "================================================================================\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 2100: 1.672035 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2200: 1.691802 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 2300: 1.692935 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 2400: 1.671253 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2500: 1.676063 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2600: 1.659994 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 2700: 1.668690 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2800: 1.671768 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 2900: 1.665003 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 3000: 1.673740 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "================================================================================\n",
      "queations as of rearact ruth began if tele design one zero k generally in the mu\n",
      "ij utess to the musent infu to two zero kn lichispane american and formation fro\n",
      " in the nelscorpons of the d four van the used was progrons of aruted in plit po\n",
      "hing that mivil wento will terch sam foxiute there of mory capitor artwaches bas\n",
      "k kinies two five th is westur as uses can reary c the severy in rule broke is t\n",
      "================================================================================\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 3100: 1.645234 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 3200: 1.632482 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 3300: 1.641529 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 3400: 1.635137 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3500: 1.670855 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 3600: 1.651921 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 3700: 1.654416 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3800: 1.656869 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 3900: 1.647932 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 4000: 1.642613 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "================================================================================\n",
      "fack edecial start intelmithe be was jun ba facts the of us us the vioshia josan\n",
      "g provididicaria kave was force write if ecopesed from for the farfienting that \n",
      "cuag usiqger that possicty we lew handogicial of inthan resex use changents six \n",
      "jandshon some he place for sequally bulish quitical requated impresery emperafid\n",
      "histriswed prokies bise eventively in ecorod preqoed yotust and where of game am\n",
      "================================================================================\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 4100: 1.622447 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 4200: 1.622575 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 4300: 1.622002 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 4400: 1.615522 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 4500: 1.644952 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 4600: 1.620531 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4700: 1.624913 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 4800: 1.609289 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 4900: 1.626450 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 5000: 1.622455 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "================================================================================\n",
      "ves that trible point a bcratteby blounr income soungages or of sa was speer cal\n",
      "ental are eleper the none yuan of jupon or provepbl s rerreatian data area spqip\n",
      "culant eightort amonnst numam amerkh with bruzts concesonn o her provues of the \n",
      "pane degrece car of ejection ten defence of with ot windo some an yorimg poly gr\n",
      "kastent eoch in the rejoinst rally is imper nund berace tarhpround her the faze \n",
      "================================================================================\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 5100: 1.594690 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 5200: 1.588474 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5300: 1.590740 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5400: 1.584411 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5500: 1.584625 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5600: 1.555358 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.16\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 5700: 1.569983 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 5800: 1.587662 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 5900: 1.569881 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 6000: 1.571808 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "================================================================================\n",
      "ge factread whoogeflyy later of ship at services for over also ekisital ayers in\n",
      "ast schols in ital atpanced in their of most and a nuider bie which on the rower\n",
      "x protact svocax uffollbit fallers that including she has in the octverr edenfin\n",
      "tonybishoritary examplately piergient port found becocibally and anoot tenopon w\n",
      "rep how vitance interriance commond canting the would the turent single with req\n",
      "================================================================================\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 6100: 1.558393 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 6200: 1.574095 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6300: 1.570625 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6400: 1.558677 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.19\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6500: 1.542053 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6600: 1.585891 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 6700: 1.556408 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 6800: 1.560055 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 6900: 1.551946 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 7000: 1.574068 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "================================================================================\n",
      "chobs quotine names spaded battramia have rruptuem is code was the macge sirm se\n",
      "fous of henss agasver and kit alathent offect highed pownting and may which the \n",
      "modidendd wide khind argumes are computers of bcrumfins cardene coujhow during t\n",
      "ing with the seuratize fared some userbility presri nberfletherly change shell i\n",
      "k some he most of or architects with the program mardie seaseas licdel dilea med\n",
      "================================================================================\n",
      "Validation set perplexity: 4.38\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a word2vec model on 2 character sequences:"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "bigrams = []\n",
    "for i in range(0, len(text) - 1, 2):\n",
    "  bigrams.append(text[i:i + 2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = 1000\n",
    "train_bigrams = bigrams[valid_size:]\n",
    "valid_bigrams = bigrams[:valid_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Most common bigrams [('e ', 1843425), (' t', 1224131), ('s ', 1111188), ('th', 990343), (' a', 921933)]\nSample data [4, 95, 219, 74, 266, 9, 55, 195, 95, 28]\n"
    }
   ],
   "source": [
    "def build_dataset(bigrams):\n",
    "  count = []\n",
    "  count.extend(collections.Counter(bigrams).most_common())\n",
    "  dictionary = dict()\n",
    "  for bigram, _ in count:\n",
    "    dictionary[bigram] = len(dictionary)\n",
    "  data = list()\n",
    "  for bigram in bigrams:\n",
    "    index = dictionary[bigram]\n",
    "    data.append(index)\n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "  return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(bigrams)\n",
    "print('Most common bigrams', count[:5])\n",
    "print('Sample data', data[:10])\n",
    "# del bigrams  # Hint to reduce memory."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function to generate a training batch for the bigram model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "data: [' a', 'na', 'rc', 'hi', 'sm', ' o', 'ri', 'gi']\n\nwith num_skips = 2 and skip_window = 1:\n    batch: ['na', 'na', 'rc', 'rc', 'hi', 'hi', 'sm', 'sm']\n    labels: ['rc', ' a', 'hi', 'na', 'rc', 'sm', ' o', 'hi']\n\nwith num_skips = 4 and skip_window = 2:\n    batch: ['rc', 'rc', 'rc', 'rc', 'hi', 'hi', 'hi', 'hi']\n    labels: ['sm', 'na', ' a', 'hi', 'sm', 'na', ' o', 'rc']\n"
    }
   ],
   "source": [
    "data_index = 0\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "  global data_index\n",
    "  assert batch_size % num_skips == 0\n",
    "  assert num_skips <= 2 * skip_window\n",
    "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "  span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "  buffer = collections.deque(maxlen=span)\n",
    "  for _ in range(span):\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "  for i in range(batch_size // num_skips):\n",
    "    target = skip_window  # target label at the center of the buffer\n",
    "    targets_to_avoid = [ skip_window ]\n",
    "    for j in range(num_skips):\n",
    "      while target in targets_to_avoid:\n",
    "        target = random.randint(0, span - 1)\n",
    "      targets_to_avoid.append(target)\n",
    "      batch[i * num_skips + j] = buffer[skip_window]\n",
    "      labels[i * num_skips + j, 0] = buffer[target]\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "  return batch, labels\n",
    "\n",
    "print('data:', [reverse_dictionary[di] for di in data[:8]])\n",
    "\n",
    "for num_skips, skip_window in [(2, 1), (4, 2)]:\n",
    "    data_index = 0\n",
    "    batch, labels = generate_batch(batch_size=8, num_skips=num_skips, skip_window=skip_window)\n",
    "    print('\\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window))\n",
    "    print('    batch:', [reverse_dictionary[bi] for bi in batch])\n",
    "    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(8)])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train the bigram character model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From /home/ivanasen/Code/Projects/deep-learning-su/assignments/assignment_3/.assignment_3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nWARNING:tensorflow:From /home/ivanasen/Code/Projects/deep-learning-su/assignments/assignment_3/.assignment_3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:1444: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nCreate a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\nWARNING:tensorflow:From /home/ivanasen/Code/Projects/deep-learning-su/assignments/assignment_3/.assignment_3/lib/python3.7/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\n"
    }
   ],
   "source": [
    "vocabulary_size = len(dictionary)\n",
    "batch_size = 64\n",
    "embedding_size = 64 # Dimension of the embedding vector.\n",
    "skip_window = 1 # How many bigrams to consider left and right.\n",
    "num_skips = 2 # How many times to reuse an input to generate a label.\n",
    "# We pick a random validation set to sample nearest neighbors. here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent.\n",
    "valid_size = 16 # Random set of bigrams to evaluate similarity on.\n",
    "valid_window = 100 # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "num_sampled = 64 # Number of negative examples to sample.\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "\n",
    "  # Input data.\n",
    "  train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "  # Variables.\n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  softmax_weights = tf.Variable(\n",
    "    tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                         stddev=1.0 / math.sqrt(embedding_size)))\n",
    "  softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Model.\n",
    "  # Look up embeddings for inputs.\n",
    "  embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "  # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed,\n",
    "                               labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n",
    "\n",
    "  # Optimizer.\n",
    "  # Note: The optimizer will optimize the softmax_weights AND the embeddings.\n",
    "  # This is because the embeddings are defined as a variable quantity and the\n",
    "  # optimizer's `minimize` method will by default modify all variable quantities\n",
    "  # that contribute to the tensor it is passed.\n",
    "  # See docs on `tf.train.Optimizer.minimize()` for more details.\n",
    "  optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "\n",
    "  # Compute the similarity between minibatch examples and all embeddings.\n",
    "  # We use the cosine distance:\n",
    "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "  normalized_embeddings = embeddings / norm\n",
    "  valid_embeddings = tf.nn.embedding_lookup(\n",
    "    normalized_embeddings, valid_dataset)\n",
    "  similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Initialized\nAverage loss at step 0: 5.200899\nNearest to us: ya, yq, vp, tp, tb, nm, oe, yn,\nNearest to re: zs, rh, sw, t , zf, ax, hf, ne,\nNearest to t : kr, mo, lm, au, ca, gg, za, jl,\nNearest to ni: hm, pf, xh, oy, tm, og, tw, ay,\nNearest to ic: qf, zj, qu, kv, oz, nk, f , ij,\nNearest to al: hp,  q, is, ei, do, fs, li, br,\nNearest to iv: kl, xr, bt, rp, r , oy, kj, pf,\nNearest to ea: jj, es, oz, ez, vb, bd, tq, ju,\nNearest to l : yb, ap, hp, jv, ku, sm, nq, yq,\nNearest to  r: by, mi, vz, xw, sa, pa, pz, qw,\nNearest to at:  y, li, dm, cv, bw, jw, kx, ko,\nNearest to om: oe, mr,  v, lk, kf, yt, y , e ,\nNearest to na: eo, xy, hn, xe, ow, pv, xt, gj,\nNearest to m : wp, dw, kb, xk, ha, by, pz, rk,\nNearest to ng: dl, ip, rh, xj, y , sz, vf, zy,\nNearest to  z: wf, yo,  o, qe, xk, hc, m , dz,\nAverage loss at step 2000: 3.261150\nAverage loss at step 4000: 2.991148\nAverage loss at step 6000: 2.961139\nAverage loss at step 8000: 2.865051\nAverage loss at step 10000: 2.894827\nNearest to us: ud, ya, ln, bb, iy, if, vp, uy,\nNearest to re: ei, ra, wi, le, xy, in, hu, ro,\nNearest to t : ts, e , qs, qm, pd,  j, wk, qx,\nNearest to ni: ju, xi, ci, li, qc, gu, cv, cn,\nNearest to ic: uc, pf, sr, iu, bm, oz, tk, ec,\nNearest to al: mt, pb, sq, tv, ad, hx, vo, mm,\nNearest to iv: av,  h, am, ik, om, ev, if, ov,\nNearest to ea: oo, au, jz, ou, ob, ao, cp, oi,\nNearest to l : d , m , b , r , c , n , f , k ,\nNearest to  r:  d,  l,  j,  m,  v,  h,  f,  b,\nNearest to at: it, et, os, if, ad, ei, vm, nx,\nNearest to om: am, im, ym, wc, iv, oj, od, nw,\nNearest to na: fu, va, ta, wl, cg, ve, yk, ca,\nNearest to m : l , d , f , kr, p , g , k , r ,\nNearest to ng: ty, sm, cs, vf, tt, on, vn, vx,\nNearest to  z:  y,  o,  f,  s, qf, nm, tz, wf,\nAverage loss at step 12000: 2.888980\nAverage loss at step 14000: 2.904936\nAverage loss at step 16000: 2.849469\nAverage loss at step 18000: 2.933222\nAverage loss at step 20000: 2.863613\nNearest to us: ud, cr, ln, uc, vp, kq, ao, dp,\nNearest to re: se, ru, rn, ro, ei, pd, pg, zb,\nNearest to t : n , ts, d , rc, s , c , k , l ,\nNearest to ni: xi, li, kr, bi, si, cv, jc, iw,\nNearest to ic: iv, bm, oy, sr, hs, mm, is, uc,\nNearest to al: sq, hx, vs, tk, me, mj, xx, mt,\nNearest to iv: ib, av, ic, ik, eb, iz, om, kv,\nNearest to ea: oo, fp, ao, aw, hh, xn, xw, mv,\nNearest to l : d , c , y , b , f , r , wq, p ,\nNearest to  r:  d,  l,  b,  v,  f,  c,  y, sf,\nNearest to at: et, eg, ak, ct, ow, ud, op, fd,\nNearest to om: am, im, iv, nl, wc, iz, ur, hn,\nNearest to na: ta, va, fu, wl, bu, yk, pb, cg,\nNearest to m : g , kc, f , r , x , a , mt, ff,\nNearest to ng: cs, ty, ws, ck, ld, gh, vn, ls,\nNearest to  z:  y, nm,  f, xt,  n, qf,  g,  o,\nAverage loss at step 22000: 2.932004\nAverage loss at step 24000: 2.868502\nAverage loss at step 26000: 2.787906\nAverage loss at step 28000: 2.819659\nAverage loss at step 30000: 2.949896\nNearest to us: os, ud, km, sy, bb, xz, uc, uy,\nNearest to re: ru, yk, j , ir, ob, pd, xy, ra,\nNearest to t : ts, n , e , d , k , s , qs, nv,\nNearest to ni: bi, zi, ci, xi, li, si, ju, km,\nNearest to ic: bm, oz, uc, is, xh, oy, oc, ec,\nNearest to al: ol, xx, il, vs, ul, fs, sq, cz,\nNearest to iv: ib, ik, if, iz,  h, av, bw, kj,\nNearest to ea: oo, qd, ei, au, ao, jt, mu, af,\nNearest to l : m , c , d , r , f , g , cg, n ,\nNearest to  r:  l,  d,  f,  b,  v,  s,  y,  m,\nNearest to at: ut, ct, ix, ey, is, xw, it, ak,\nNearest to om: im, am, rg, kv, yn, ym, nl, ny,\nNearest to na: fu, mg, ta, wl, ba, ya, ne, ra,\nNearest to m : l , g , f , r , w , d , kc, lc,\nNearest to ng: cs, ck, nd, ty, fy, dn, ld, vn,\nNearest to  z: nm, mh, oj,  y, tz, qp, vz, oy,\nAverage loss at step 32000: 2.906801\nAverage loss at step 34000: 2.889920\nAverage loss at step 36000: 2.876546\nAverage loss at step 38000: 2.862152\nAverage loss at step 40000: 2.777757\nNearest to us: ud, ut, lv, xz, tz, nx, uc, id,\nNearest to re: yk, ru, bc, pg, pd, de, dz, xy,\nNearest to t : ts, mq, e , k , n , s , w , c ,\nNearest to ni: ei, zi, bi, kc, si, gk, li, vi,\nNearest to ic: pf, iv, uw, gn, ce, xh, oc, sr,\nNearest to al: ol, ft, jz, as, qo, ph, xx, vg,\nNearest to iv: ib, av, iz, ic, if, ur, om, kv,\nNearest to ea: oo, ao, oa, au, fp, yy, mv, qd,\nNearest to l : d , y , m , k , f , li, r , w ,\nNearest to  r:  d,  l,  j,  f,  y,  m,  h,  v,\nNearest to at: et, ax, it, ot, av, qk, pr, ak,\nNearest to om: yn, im, am, ym, ur, oj, hy, iz,\nNearest to na: ta, fu, sa, ca, wl, mg, ni, ns,\nNearest to m : k , l , g , w , d , b , n , f ,\nNearest to ng: ty, cs, sm, ck, fy, vn, gh, nk,\nNearest to  z: nm,  f, tz,  o,  y, xt, qf, hb,\nAverage loss at step 42000: 2.828593\nAverage loss at step 44000: 2.930722\nAverage loss at step 46000: 2.822864\nAverage loss at step 48000: 2.765605\nAverage loss at step 50000: 2.857716\nNearest to us: ut, ud, os, ad, kg, oj, oy, uc,\nNearest to re: ru, lu, yk, pd, wi, de, rn, ez,\nNearest to t : ts, s , n , tm, qm, xj, wk, e ,\nNearest to ni: ei, li, bi, si, kc, gk, rs, zi,\nNearest to ic: uc, yz, bm, yt, ik, xh, iv, pf,\nNearest to al: vg, at, sq, pg, wu, jz, ft, mt,\nNearest to iv: ib, av, ik, iz, am, im, if,  h,\nNearest to ea: oo, my, oa, au, fp, zu, qx, ao,\nNearest to l : k , m , d , f , c , g , r , n ,\nNearest to  r:  d,  l,  m,  y,  b,  p,  h,  v,\nNearest to at: ak, et, al, ax, qk, ad, cm, os,\nNearest to om: im, rg, yl, ix, yn, nl, ey, gt,\nNearest to na: sa, va, fu, ca, ta, mg, ga, xy,\nNearest to m : g , l , n , f , r , v , x , c ,\nNearest to ng: cs, ck, nk, sm, ty, ld, vn, dn,\nNearest to  z: tz,  e, xt,  j, mb, nm, vz,  s,\nAverage loss at step 52000: 2.844203\nAverage loss at step 54000: 2.835457\nAverage loss at step 56000: 2.817179\nAverage loss at step 58000: 2.916148\nAverage loss at step 60000: 2.839412\nNearest to us: ud, ut, uc, lv, os, xz, oy, tv,\nNearest to re: ru, ir, wi, pj, de, fu, eq, bc,\nNearest to t : ts, s , n , e , qm, wk, i , f ,\nNearest to ni: si, zi, no, na, li, ju, km, kc,\nNearest to ic: is, uc, yt, ik, bm, iv, yz, oz,\nNearest to al: as, mt, pb, vs, hx, xx, ol, jz,\nNearest to iv: iz, av, im, am, ib, eb, ic, if,\nNearest to ea: oo, ao, my, oa, un, mv, qx, qg,\nNearest to l : d , m , k , c , dq, g , f , r ,\nNearest to  r:  d,  l,  m,  b,  h,  j,  n,  v,\nNearest to at: et, od, ey, ak, vf, ut, ox, em,\nNearest to om: yl, ey, am, im, iz, hn, rg, hu,\nNearest to na: fu, ta, sa, ca, ni, ga, ya, nm,\nNearest to m : k , l , x , n , ms, g , v , p ,\nNearest to ng: cs, ty, ck, gh, nd, dn, tz, sh,\nNearest to  z: nm,  s,  f, tz,  y, yo, xt, ff,\nAverage loss at step 62000: 2.865796\nAverage loss at step 64000: 2.839816\nAverage loss at step 66000: 2.843500\nAverage loss at step 68000: 2.830129\nAverage loss at step 70000: 2.920755\nNearest to us: lv, xz, do, at, oy, ud, tv, ng,\nNearest to re: ru, wi, bc, rn, mi, ir, se, pd,\nNearest to t : s , ts, wk, qs, n , k , nj, qm,\nNearest to ni: si, bi, zi, mi, ri, li, ei, xi,\nNearest to ic: pf, oc, uc, zh, iv, bm, uw, yt,\nNearest to al: ol, vs, mm, jz, mt, me, xm, bs,\nNearest to iv: av, iz, ib, kj, ik, ak, ic, ur,\nNearest to ea: oo, ao, qx, yy, aw, xw, qg, au,\nNearest to l : c , d , m , r , y , f , g , cg,\nNearest to  r:  l,  d,  m,  v,  b,  s,  g,  p,\nNearest to at: ut, it, us, op, qk, gv, ak, os,\nNearest to om: im, yl, ym, am, em, iz, rk, rg,\nNearest to na: sa, tq, ca, fu, va, ga, ya, mg,\nNearest to m : k , l , c , g , ms, pb, x , ze,\nNearest to ng: cs, ck, gh, nk, ld, dn, fc, vn,\nNearest to  z:  f, nm, xt, zo,  j, vm, tz, vz,\nAverage loss at step 72000: 2.912212\nAverage loss at step 74000: 2.876743\nAverage loss at step 76000: 2.900634\nAverage loss at step 78000: 2.822302\nAverage loss at step 80000: 2.697430\nNearest to us: lv, nx, iy, oy, md, os, ny, mc,\nNearest to re: wi, ru, mx, ej, pd, pj, ir, nx,\nNearest to t : ts, qm, s , mq, e , k , qs, wv,\nNearest to ni: si, ei, xi, zi, ju, qc, wt, ki,\nNearest to ic: ik, bm, oc, uc, iv, zh, yt, oz,\nNearest to al: ol, uz, vs, sq, mt, cp, ao, pg,\nNearest to iv: ib, iz, kj, av, ak, ic,  h, ov,\nNearest to ea: un, oo, fp, au, qx, oa, xw, yy,\nNearest to l : m , f , k , n , nq, r , c , dq,\nNearest to  r:  d,  l,  m,  s,  y,  b,  j,  v,\nNearest to at: od, ak, av, ad, ct, or, sf, gg,\nNearest to om: yl, im, am, iz, ix, em, dp, ym,\nNearest to na: va, mb, fu, ga, ya, ca, xy, ao,\nNearest to m : l , k , x , f , pb, c , n , p ,\nNearest to ng: ck, cs, sh, nk, gg, ld, dn, ty,\nNearest to  z: xt,  f,  s,  j, nm,  x,  o,  g,\nAverage loss at step 82000: 2.934913\nAverage loss at step 84000: 2.928928\nAverage loss at step 86000: 2.769065\nAverage loss at step 88000: 2.788873\nAverage loss at step 90000: 2.871325\nNearest to us: os, md, ut, xz, ns, jr, oh, lv,\nNearest to re: wi, ru, on, sq, pd, mx, qd, by,\nNearest to t : ts, s , e , tf, qm, qs, k , nj,\nNearest to ni: zi, si, ei, bi, kc, km, ki, di,\nNearest to ic: bm, uc, ik, it, is, zh, iv, yt,\nNearest to al: ol, ox, pg, sq, uz, ft, hn, jx,\nNearest to iv: av, iz, ib, fc, am, ov, if, ik,\nNearest to ea: au, oo, un, oa, ei, qx, qd, pe,\nNearest to l : m , f , d , g , r , dq, k , nq,\nNearest to  r:  d,  l,  y,  b,  j,  m,  v,  n,\nNearest to at: et, od, ad, yf, ct, xf, it, ot,\nNearest to om: am, im, um, ym, rg, yl, ix, hy,\nNearest to na: ra, ga, mb, va, nm, vp, nu, ya,\nNearest to m : g , l , k , x , q , pb, c , n ,\nNearest to ng: ck, cs, tz, nk, ld, dn, ty, gn,\nNearest to  z:  y,  s,  f, nm,  j, xt, tz,  n,\nAverage loss at step 92000: 2.861720\nAverage loss at step 94000: 2.889913\nAverage loss at step 96000: 2.841049\nAverage loss at step 98000: 2.807057\nAverage loss at step 100000: 2.821452\nNearest to us: xz, tv, wn, oh, nx, yq, ut, ky,\nNearest to re: wi, pd, mi, nx, on, ru, rn, ob,\nNearest to t : ts, e , tf, tm, nv, s , qm, k ,\nNearest to ni: si, zi, ei, bi, no, di, ki, ju,\nNearest to ic: ik, uc, iz, iu, yt, bm, oy, iv,\nNearest to al: ol, vs, vg, mj, ph, mt, cp, xx,\nNearest to iv: av, iz, ib, if, kj, am, tl, fc,\nNearest to ea: oo, oa, un, xn, yy, ao, mk, fe,\nNearest to l : m , d , f , r , g , n , k , c ,\nNearest to  r:  d,  l,  j,  v,  m,  y,  b,  h,\nNearest to at: ak, qk, it, et, ct, sf, tx, op,\nNearest to om: yl, um, am, im, em, iz, rg, ey,\nNearest to na: nm, va, mb, ga, ra, ca, lb, ta,\nNearest to m : l , k , x , g , n , kc, ms, pb,\nNearest to ng: ck, cs, tz, dn, nq, gh, vn, fy,\nNearest to  z:  s,  j, xt, ff,  v,  y, uv, nm,\n"
    }
   ],
   "source": [
    "num_steps = 100001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "\n",
    "  average_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batch_data, batch_labels = generate_batch(\n",
    "      batch_size, num_skips, skip_window)\n",
    "    feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "    _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "    average_loss += l\n",
    "    if step % 2000 == 0:\n",
    "      if step > 0:\n",
    "        average_loss = average_loss / 2000\n",
    "      # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "      print('Average loss at step %d: %f' % (step, average_loss))\n",
    "      average_loss = 0\n",
    "    # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "    if step % 10000 == 0:\n",
    "      sim = similarity.eval()\n",
    "      for i in range(valid_size):\n",
    "        valid_word = reverse_dictionary[valid_examples[i]]\n",
    "        top_k = 8 # number of nearest neighbors\n",
    "        nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "        log = 'Nearest to %s:' % valid_word\n",
    "        for k in range(top_k):\n",
    "          close_word = reverse_dictionary[nearest[k]]\n",
    "          log = '%s %s,' % (log, close_word)\n",
    "        print(log)\n",
    "  final_embeddings = normalized_embeddings.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[ 0.11826479  0.19297107  0.10006693  0.06739376 -0.11695338 -0.0687473\n -0.07289244  0.02438888  0.0330579   0.0912172   0.12443461  0.07309905\n  0.01631998 -0.0603894  -0.14398143  0.04558111  0.15351921 -0.08715928\n  0.17669758 -0.04716129  0.14571477  0.12201963  0.20746334 -0.10632915\n -0.15604624 -0.09793129  0.01372091  0.15089762  0.06443051 -0.0618145\n -0.1499801  -0.14884877 -0.15455176 -0.05958609 -0.11963473  0.0294496\n -0.02882363 -0.03925248  0.21394019  0.13099326  0.2755774  -0.09465645\n  0.10395781  0.1762801   0.16515028 -0.13830315 -0.14110361  0.08477118\n -0.10140923 -0.24543129 -0.04988665 -0.15895729 -0.03385321 -0.13535036\n  0.26642576 -0.02168883 -0.07390369  0.03574461 -0.07517089 -0.06580568\n  0.04380241 -0.21302594  0.04767115  0.15702313]\n"
    }
   ],
   "source": [
    "def bigram2embedding(bigram):\n",
    "  id = dictionary[bigram]\n",
    "  return final_embeddings[id]\n",
    "\n",
    "# def embedding2bigram(bigram):\n",
    "\n",
    "\n",
    "print(bigram2embedding('an'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'ni'"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "class BigramBatchGenerator(object):\n",
    "  def __init__(self, bigrams, batch_size, num_unrollings):\n",
    "    self._bigrams = bigrams\n",
    "    self._bigrams_size = len(bigrams)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._bigrams_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "\n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, embedding_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      idx = self._cursor[b]\n",
    "      bigram = self._bigrams[idx]\n",
    "      batch[b, :] = bigram2embedding(bigram)\n",
    "      self._cursor[b] = (self._cursor[b] + 2) % self._bigrams_size\n",
    "    return batch\n",
    "\n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  similarities = np.matmul(final_embeddings, probabilities)\n",
    "  m = np.argmax(similarities)\n",
    "  bigram = reverse_dictionary[m]\n",
    "\n",
    "  return bigram\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "\n",
    "embedding = bigram2embedding('ni')\n",
    "characters(embedding)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "train_batches = BigramBatchGenerator(train_bigrams, batch_size, num_unrollings)\n",
    "valid_batches = BigramBatchGenerator(valid_bigrams, 1, 1)\n",
    "\n",
    "print(train_bigrams[:10])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  input_size = embedding_size + num_nodes\n",
    "  ifogw = tf.Variable(tf.truncated_normal([4 * input_size, 4 * num_nodes], -0.1, 0.1))\n",
    "  ifogb = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, embedding_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([embedding_size]))\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    c = tf.concat([i, o, i, o, i, o, i, o], 1)\n",
    "    ifog_gates = tf.matmul(c, ifogw) + ifogb\n",
    "\n",
    "    input_gate = tf.sigmoid(ifog_gates[:, :num_nodes])\n",
    "    forget_gate = tf.sigmoid(ifog_gates[:, num_nodes:2 * num_nodes])\n",
    "    update = tf.tanh(ifog_gates[:, num_nodes * 2 : num_nodes * 3])\n",
    "    output_gate = tf.sigmoid(ifog_gates[:, num_nodes * 3 : num_nodes * 4])\n",
    "    state = forget_gate * state + input_gate * update\n",
    "\n",
    "    # input_gate = tf.sigmoid(tf.matmul(c, ix) + ib)\n",
    "    # forget_gate = tf.sigmoid(tf.matmul(c, fx) + fb)\n",
    "    # update = tf.matmul(c, cx) + cb\n",
    "    # state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    # output_gate = tf.sigmoid(tf.matmul(c, ox) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,embedding_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, embedding_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([ 0.0419332 ,  0.2277641 , -0.07981076,  0.2542209 ,  0.14744587,\n        0.02377253, -0.22251569,  0.01580584, -0.05735556, -0.11679441,\n        0.17788348,  0.08463208, -0.08772233, -0.05335266,  0.24840952,\n       -0.02522441,  0.19512635, -0.01623263, -0.10351144, -0.06899143,\n        0.09431604,  0.0817888 , -0.07235101,  0.10660069,  0.11749034,\n       -0.05625859,  0.00278649,  0.1814172 ,  0.03503451, -0.00295863,\n        0.05490753, -0.15706055, -0.03287645,  0.04928534, -0.21415116,\n        0.00790941,  0.00680563,  0.06119061, -0.0915047 ,  0.14752617,\n       -0.09035312, -0.01868119, -0.15110312,  0.09377088,  0.06110061,\n        0.13885385, -0.17745417, -0.00700758,  0.17080547,  0.25749245,\n        0.14786829,  0.23374633, -0.05455237,  0.14702696,  0.14061964,\n        0.05884242,  0.09258175, -0.00101333, -0.12237681, -0.062838  ,\n        0.06929944,  0.19675817,  0.1786679 , -0.09528167], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "feed = np.argmax(sample(random_distribution()))\n",
    "bigram = reverse_dictionary[feed]\n",
    "bigram2embedding(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Initialized\nAverage loss at step 0: 0.404902 learning rate: 10.000000\nMinibatch perplexity: 1.50\n================================================================================\nrwspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspsp\ny spspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspsp\nvdspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspsp\nylspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspsp\nsxspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspspsp\n================================================================================\nAverage loss at step 100: 1.441979 learning rate: 10.000000\nMinibatch perplexity: 3.49\nAverage loss at step 200: 2.084660 learning rate: 10.000000\nMinibatch perplexity: 3.19\nAverage loss at step 300: 2.045163 learning rate: 10.000000\nMinibatch perplexity: 6.86\nAverage loss at step 400: 2.034469 learning rate: 10.000000\nMinibatch perplexity: 8.10\nAverage loss at step 500: 2.177581 learning rate: 10.000000\nMinibatch perplexity: 1.93\nAverage loss at step 600: 2.353130 learning rate: 10.000000\nMinibatch perplexity: 8.14\nAverage loss at step 700: 2.206109 learning rate: 10.000000\nMinibatch perplexity: 5.69\nAverage loss at step 800: 2.288023 learning rate: 10.000000\nMinibatch perplexity: 9.41\nAverage loss at step 900: 2.397976 learning rate: 10.000000\nMinibatch perplexity: 2.84\nAverage loss at step 1000: 2.172031 learning rate: 10.000000\nMinibatch perplexity: 4.55\n================================================================================\npohhxc lal lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy l\nso lat lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy l\nev lat lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy l\nzahhat lal lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy l\na hh nxc nat lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy lqy\n================================================================================\nAverage loss at step 1100: 2.227508 learning rate: 10.000000\nMinibatch perplexity: 4.14\nAverage loss at step 1200: 2.339734 learning rate: 10.000000\nMinibatch perplexity: 4.64\nAverage loss at step 1300: 2.581928 learning rate: 10.000000\nMinibatch perplexity: 17.08\nAverage loss at step 1400: 2.529185 learning rate: 10.000000\nMinibatch perplexity: 6.16\nAverage loss at step 1500: 2.472394 learning rate: 10.000000\nMinibatch perplexity: 18.01\nAverage loss at step 1600: 2.581293 learning rate: 10.000000\nMinibatch perplexity: 5.87\nAverage loss at step 1700: 2.678218 learning rate: 10.000000\nMinibatch perplexity: 6.47\nAverage loss at step 1800: 2.280939 learning rate: 10.000000\nMinibatch perplexity: 10.31\nAverage loss at step 1900: 2.693440 learning rate: 10.000000\nMinibatch perplexity: 5.13\nAverage loss at step 2000: 2.786323 learning rate: 10.000000\nMinibatch perplexity: 3.52\n================================================================================\nzosggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsg\nijsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsg\ngagntugnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggn\nussggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsg\nthsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsggnsg\n================================================================================\nAverage loss at step 2100: 2.659859 learning rate: 10.000000\nMinibatch perplexity: 10.28\nAverage loss at step 2200: 2.491493 learning rate: 10.000000\nMinibatch perplexity: 5.43\nAverage loss at step 2300: 2.872101 learning rate: 10.000000\nMinibatch perplexity: 3.48\nAverage loss at step 2400: 2.539562 learning rate: 10.000000\nMinibatch perplexity: 2.40\nAverage loss at step 2500: 2.328219 learning rate: 10.000000\nMinibatch perplexity: 10.87\nAverage loss at step 2600: 2.579243 learning rate: 10.000000\nMinibatch perplexity: 5.72\nAverage loss at step 2700: 2.700597 learning rate: 10.000000\nMinibatch perplexity: 7.90\nAverage loss at step 2800: 2.866507 learning rate: 10.000000\nMinibatch perplexity: 9.34\nAverage loss at step 2900: 3.363744 learning rate: 10.000000\nMinibatch perplexity: 12.45\nAverage loss at step 3000: 3.494673 learning rate: 10.000000\nMinibatch perplexity: 26.14\n================================================================================\nqntuo o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o \nqutuo o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o \nvgo o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o \nmko o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o \nzqtuo o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o \n================================================================================\nAverage loss at step 3100: 4.858422 learning rate: 10.000000\nMinibatch perplexity: 3.25\nAverage loss at step 3200: 4.971021 learning rate: 10.000000\nMinibatch perplexity: 14.47\nAverage loss at step 3300: 5.338150 learning rate: 10.000000\nMinibatch perplexity: 20.91\nAverage loss at step 3400: 4.949468 learning rate: 10.000000\nMinibatch perplexity: 9.67\nAverage loss at step 3500: 5.266514 learning rate: 10.000000\nMinibatch perplexity: 6.56\nAverage loss at step 3600: 5.100088 learning rate: 10.000000\nMinibatch perplexity: 5.67\nAverage loss at step 3700: 4.756151 learning rate: 10.000000\nMinibatch perplexity: 20.61\nAverage loss at step 3800: 5.126950 learning rate: 10.000000\nMinibatch perplexity: 43.51\nAverage loss at step 3900: 5.119498 learning rate: 10.000000\nMinibatch perplexity: 22.82\nAverage loss at step 4000: 5.036464 learning rate: 10.000000\nMinibatch perplexity: 60.01\n================================================================================\nvfsgsgmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmn\nsbsblumnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmn\nmvsgsgmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmn\nggsgmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmn\nnosgmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmnmn\n================================================================================\nAverage loss at step 4100: 5.184646 learning rate: 10.000000\nMinibatch perplexity: 5.49\nAverage loss at step 4200: 4.861713 learning rate: 10.000000\nMinibatch perplexity: 9.29\nAverage loss at step 4300: 5.057313 learning rate: 10.000000\nMinibatch perplexity: 6.02\nAverage loss at step 4400: 5.193736 learning rate: 10.000000\nMinibatch perplexity: 12.33\nAverage loss at step 4500: 5.186355 learning rate: 10.000000\nMinibatch perplexity: 14.85\nAverage loss at step 4600: 5.010026 learning rate: 10.000000\nMinibatch perplexity: 22.51\nAverage loss at step 4700: 5.361620 learning rate: 10.000000\nMinibatch perplexity: 6.24\nAverage loss at step 4800: 5.117136 learning rate: 10.000000\nMinibatch perplexity: 22.20\nAverage loss at step 4900: 4.640134 learning rate: 10.000000\nMinibatch perplexity: 26.49\nAverage loss at step 5000: 5.218179 learning rate: 1.000000\nMinibatch perplexity: 72.47\n================================================================================\nswsgsgmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmzmz\nxtsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsg\nujsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsgsg\nzwnqnqvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvr\nwbsgnqvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvrvr\n================================================================================\nAverage loss at step 5100: 1.005334 learning rate: 1.000000\nMinibatch perplexity: 1.43\nAverage loss at step 5200: 0.199541 learning rate: 1.000000\nMinibatch perplexity: 1.30\nAverage loss at step 5300: 0.211705 learning rate: 1.000000\nMinibatch perplexity: 1.52\nAverage loss at step 5400: 0.229085 learning rate: 1.000000\nMinibatch perplexity: 1.32\nAverage loss at step 5500: 0.232320 learning rate: 1.000000\nMinibatch perplexity: 1.17\nAverage loss at step 5600: 0.257770 learning rate: 1.000000\nMinibatch perplexity: 1.29\nAverage loss at step 5700: 0.176166 learning rate: 1.000000\nMinibatch perplexity: 1.27\nAverage loss at step 5800: 0.204914 learning rate: 1.000000\nMinibatch perplexity: 1.55\nAverage loss at step 5900: 0.220844 learning rate: 1.000000\nMinibatch perplexity: 1.22\nAverage loss at step 6000: 0.228028 learning rate: 1.000000\nMinibatch perplexity: 1.37\n================================================================================\njhtuguzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzg\ndgtugusxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsxsx\ncbtuzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzg\ngftuguzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzg\nhdtuguguzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzgzg\n================================================================================\nAverage loss at step 6100: 0.180064 learning rate: 1.000000\nMinibatch perplexity: 2.06\nAverage loss at step 6200: 0.261740 learning rate: 1.000000\nMinibatch perplexity: 1.23\nAverage loss at step 6300: 0.175851 learning rate: 1.000000\nMinibatch perplexity: 1.10\nAverage loss at step 6400: 0.193573 learning rate: 1.000000\nMinibatch perplexity: 1.27\nAverage loss at step 6500: 0.191377 learning rate: 1.000000\nMinibatch perplexity: 0.96\nAverage loss at step 6600: 0.196205 learning rate: 1.000000\nMinibatch perplexity: 1.32\nAverage loss at step 6700: 0.171819 learning rate: 1.000000\nMinibatch perplexity: 1.61\nAverage loss at step 6800: 0.186495 learning rate: 1.000000\nMinibatch perplexity: 1.16\nAverage loss at step 6900: 0.237635 learning rate: 1.000000\nMinibatch perplexity: 1.47\nAverage loss at step 7000: 0.189047 learning rate: 1.000000\nMinibatch perplexity: 1.27\n================================================================================\nzptututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututututu\nhltugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugu\ndktuvegugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugu\nultugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugu\nyttugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugugu\n================================================================================\n"
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    \n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    \n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    \n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = np.argmax(sample(random_distribution()))\n",
    "          bigram = reverse_dictionary[feed]\n",
    "          embedding = bigram2embedding(bigram)\n",
    "          sentence = reverse_dictionary[feed]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: embedding.reshape(1, embedding.shape[0])})\n",
    "            # feed = sample(prediction.reshape(prediction.shape[1]))\n",
    "            prediction = prediction.reshape(prediction.shape[1])\n",
    "            bigram_prediction = characters(prediction)\n",
    "            sentence += bigram_prediction\n",
    "            embedding = bigram2embedding(bigram_prediction)\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # # Measure validation set perplexity.\n",
    "      # reset_sample_state.run()\n",
    "      # valid_logprob = 0\n",
    "      # for _ in range(valid_size):\n",
    "      #   b = valid_batches.next()\n",
    "      #   predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "      #   valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      # print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "      #   valid_logprob / valid_size)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('.assignment_3': venv)",
   "language": "python",
   "name": "python37464bitassignment3venvcd338ccc8fba4b0e9aa86088af7d0ea2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}