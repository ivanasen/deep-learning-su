{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4embtkV0pNxM"
   },
   "source": [
    "Deep Learning with Tensorflow\n",
    "=============\n",
    "\n",
    "Assignment II\n",
    "------------\n",
    "\n",
    "During one of the lectures in [Lab 1](https://deep-learning-su.github.io/labs/lab-1/) we trained fully connected network to classify [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) characters. \n",
    "\n",
    "The goal of this assignment is make the neural network convolutional.\n",
    "\n",
    "For this exercise, you would need the `notMNIST.pickle` created in `Lab 1`. You can obtain it by rerunning the given paragraphs without having to solve the problems (although it is highly recommended to do it if you haven't already)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "both",
    "colab_type": "code",
    "id": "tm2CQN_Cpwj0",
    "colab": {}
   },
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import os"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "both",
    "colab_type": "code",
    "id": "y3-cj1bpmuxc",
    "colab": {},
    "tags": []
   },
   "source": [
    "data_dir = 'data/'\n",
    "pickle_file = os.path.join(data_dir, 'notMNIST.pickle')\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Training set (200000, 28, 28) (200000,)\nValidation set (10000, 28, 28) (10000,)\nTest set (10000, 28, 28) (10000,)\n"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a TensorFlow-friendly shape:\n",
    "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "both",
    "colab_type": "code",
    "id": "IRSyYiIIGIzS",
    "colab": {},
    "tags": []
   },
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Training set (200000, 28, 28, 1) (200000, 10)\nValidation set (10000, 28, 28, 1) (10000, 10)\nTest set (10000, 28, 28, 1) (10000, 10)\n"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "both",
    "colab_type": "code",
    "id": "AgQDIREv02p1",
    "colab": {}
   },
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rhgjmROXu2O"
   },
   "source": [
    "## Problem 1\n",
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes.\n",
    "\n",
    "Edit the snippet bellow by changing the `model` function.\n",
    "\n",
    "### 1.1 - Define the model\n",
    "Implement the `model` function bellow. Take a look at the following TF functions:\n",
    "- **tf.nn.conv2d(X,W1, strides = [1,s,s,1], padding = 'SAME'):** given an input $X$ and a group of filters $W1$, this function convolves $W1$'s filters on X. The third input ([1,f,f,1]) represents the strides for each dimension of the input (m, n_H_prev, n_W_prev, n_C_prev). You can read the full documentation [here](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d)\n",
    "- **tf.nn.relu(Z1):** computes the elementwise ReLU of Z1 (which can be any shape). You can read the full documentation [here.](https://www.tensorflow.org/api_docs/python/tf/nn/relu)\n",
    "\n",
    "### 1.2 - Compute loss\n",
    "\n",
    "Implement the `compute_loss` function below. You might find these two functions helpful: \n",
    "\n",
    "- **tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y):** computes the softmax entropy loss. This function both computes the softmax activation function as well as the resulting loss. You can check the full documentation  [here.](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits)\n",
    "- **tf.reduce_mean:** computes the mean of elements across dimensions of a tensor. Use this to sum the losses over all the examples to get the overall cost. You can check the full documentation [here.](https://www.tensorflow.org/api_docs/python/tf/reduce_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  # uses default std. deviation\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  # uses default bias\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def compute_loss(labels, logits):\n",
    "    entropy_loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "    return tf.reduce_mean(entropy_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "both",
    "colab_type": "code",
    "id": "IZYv70SvvOan",
    "colab": {},
    "tags": []
   },
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16 # Number of filters?\n",
    "num_hidden = 64 # Size of the fully connected layer?\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "  # Convolutional Layer 1\n",
    "  W_conv_1 = weight_variable([patch_size, patch_size, 1, depth])\n",
    "  W_bias_1 = bias_variable([depth])\n",
    "\n",
    "  # Convolutional Layer 2\n",
    "  W_conv_2 = weight_variable([patch_size, patch_size, depth, depth])\n",
    "  W_bias_2 = bias_variable([depth])\n",
    "  \n",
    "  # Dense Layer 1\n",
    "  W_dense_1 = weight_variable([7 * 7 * 16, num_hidden])\n",
    "  W_dense_1_bias = bias_variable([num_hidden])\n",
    "\n",
    "  # Dense Layer 2\n",
    "  W_dense_2 = weight_variable([num_hidden, num_labels])\n",
    "  W_dense_2_bias = bias_variable([num_labels])\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    # define a simple network with \n",
    "    # * 2 convolutional layers with 5x5 filters each using stride 2 and zero padding\n",
    "    # * one fully connected layer\n",
    "    # return the logits (last layer)\n",
    "\n",
    "    conv_1_out = tf.nn.relu(tf.nn.conv2d(data, W_conv_1, [1, 2, 2, 1], 'SAME') + W_bias_1)\n",
    "    conv_2_out = tf.nn.relu(tf.nn.conv2d(conv_1_out, W_conv_2, [1, 2, 2, 1], 'SAME') + W_bias_2)\n",
    "\n",
    "    # Flatten\n",
    "    flat = tf.reshape(conv_2_out, [-1, 7 * 7 * 16])\n",
    "\n",
    "    dense_1_out = tf.nn.relu(tf.matmul(flat, W_dense_1) + W_dense_1_bias)   \n",
    "    logits = tf.matmul(dense_1_out, W_dense_2) + W_dense_2_bias\n",
    "\n",
    "    return logits\n",
    "\n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = compute_loss(tf_train_labels, logits)\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From /home/ivanasen/Tools/anaconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nWARNING:tensorflow:From <ipython-input-6-fddc41529dbc>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee `tf.nn.softmax_cross_entropy_with_logits_v2`.\n\n"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZkzpbHET-m8S",
    "colab_type": "text"
   },
   "source": [
    "### 1.3 - Measure the accuracy and tune your model\n",
    "\n",
    "Run the snippet bellow to measure the accuracy of your model. Try to achieve a test accuracy of around 80%. Iterate on the filters size."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "both",
    "colab_type": "code",
    "id": "noKFb2UovVFR",
    "colab": {},
    "tags": []
   },
   "source": [
    "num_steps = 1001\n",
    "\n",
    "def train(graph, optimizer, train_prediction, valid_prediction, test_prediction):\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    print('Initialized')\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "      if (step % 50 == 0):\n",
    "        print(f'Minibatch loss at step {step}: {l}')\n",
    "        print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "        print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "train(graph, optimizer, train_prediction, valid_prediction, test_prediction)"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Initialized\nMinibatch loss at step 0: 2.455214262008667\nMinibatch accuracy: 6.2%\nValidation accuracy: 11.4%\nMinibatch loss at step 50: 1.0963480472564697\nMinibatch accuracy: 56.2%\nValidation accuracy: 55.8%\nMinibatch loss at step 100: 0.524612545967102\nMinibatch accuracy: 93.8%\nValidation accuracy: 75.1%\nMinibatch loss at step 150: 1.0666847229003906\nMinibatch accuracy: 68.8%\nValidation accuracy: 77.2%\nMinibatch loss at step 200: 0.4244617223739624\nMinibatch accuracy: 87.5%\nValidation accuracy: 78.1%\nMinibatch loss at step 250: 1.2042909860610962\nMinibatch accuracy: 68.8%\nValidation accuracy: 78.4%\nMinibatch loss at step 300: 0.722869873046875\nMinibatch accuracy: 81.2%\nValidation accuracy: 80.3%\nMinibatch loss at step 350: 0.6444584131240845\nMinibatch accuracy: 81.2%\nValidation accuracy: 79.1%\nMinibatch loss at step 400: 0.40057799220085144\nMinibatch accuracy: 87.5%\nValidation accuracy: 80.5%\nMinibatch loss at step 450: 0.37450891733169556\nMinibatch accuracy: 87.5%\nValidation accuracy: 80.8%\nMinibatch loss at step 500: 0.8148146271705627\nMinibatch accuracy: 81.2%\nValidation accuracy: 81.8%\nMinibatch loss at step 550: 0.32749730348587036\nMinibatch accuracy: 87.5%\nValidation accuracy: 81.8%\nMinibatch loss at step 600: 0.7292375564575195\nMinibatch accuracy: 75.0%\nValidation accuracy: 80.8%\nMinibatch loss at step 650: 1.1577502489089966\nMinibatch accuracy: 81.2%\nValidation accuracy: 82.5%\nMinibatch loss at step 700: 0.41987937688827515\nMinibatch accuracy: 93.8%\nValidation accuracy: 82.3%\nMinibatch loss at step 750: 0.8176343441009521\nMinibatch accuracy: 68.8%\nValidation accuracy: 81.8%\nMinibatch loss at step 800: 0.707410454750061\nMinibatch accuracy: 75.0%\nValidation accuracy: 83.5%\nMinibatch loss at step 850: 0.1391899138689041\nMinibatch accuracy: 93.8%\nValidation accuracy: 83.2%\nMinibatch loss at step 900: 1.431060552597046\nMinibatch accuracy: 56.2%\nValidation accuracy: 83.4%\nMinibatch loss at step 950: 0.24235235154628754\nMinibatch accuracy: 93.8%\nValidation accuracy: 83.7%\nMinibatch loss at step 1000: 0.40108445286750793\nMinibatch accuracy: 81.2%\nValidation accuracy: 83.5%\nTest accuracy: 89.5%\n"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KedKkn4EutIK"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (`nn.max_pool()`) of stride 2 and kernel size 2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16 # Number of filters?\n",
    "num_hidden = 64 # Size of the fully connected layer?\n",
    "\n",
    "graph_pool = tf.Graph()\n",
    "\n",
    "with graph_pool.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "  # Convolutional Layer 1\n",
    "  W_conv_1 = weight_variable([patch_size, patch_size, 1, depth])\n",
    "  W_bias_1 = bias_variable([depth])\n",
    "\n",
    "  # Convolutional Layer 2\n",
    "  W_conv_2 = weight_variable([patch_size, patch_size, depth, depth])\n",
    "  W_bias_2 = bias_variable([depth])\n",
    "  \n",
    "  # Dense Layer 1\n",
    "  W_dense_1 = weight_variable([7 * 7 * 16, num_hidden])\n",
    "  W_dense_1_bias = bias_variable([num_hidden])\n",
    "\n",
    "  # Dense Layer 2\n",
    "  W_dense_2 = weight_variable([num_hidden, num_labels])\n",
    "  W_dense_2_bias = bias_variable([num_labels])\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    # define a simple network with \n",
    "    # * 2 convolutional layers with 5x5 filters each using stride 2 and zero padding\n",
    "    # * one fully connected layer\n",
    "    # return the logits (last layer)\n",
    "\n",
    "    # print(data.shape)\n",
    "\n",
    "    conv_1_out = tf.nn.relu(tf.nn.conv2d(data, W_conv_1, [1, 1, 1, 1], 'SAME') + W_bias_1)\n",
    "    pool_1_out = tf.nn.max_pool(conv_1_out, [1, 2, 2, 1], [1, 2, 2, 1], 'SAME')\n",
    "    \n",
    "    conv_2_out = tf.nn.relu(tf.nn.conv2d(pool_1_out, W_conv_2, [1, 1, 1, 1], 'SAME') + W_bias_2)\n",
    "    pool_2_out = tf.nn.max_pool(conv_2_out, [1, 2, 2, 1], [1, 2, 2, 1], 'SAME')    \n",
    "\n",
    "    # Flatten\n",
    "    flat = tf.reshape(pool_2_out, [-1, 7 * 7 * 16])\n",
    "\n",
    "    dense_1_out = tf.nn.relu(tf.matmul(flat, W_dense_1) + W_dense_1_bias)   \n",
    "    logits = tf.matmul(dense_1_out, W_dense_2) + W_dense_2_bias\n",
    "\n",
    "    return logits\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = compute_loss(tf_train_labels, logits)\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.005).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Initialized\nMinibatch loss at step 0: 2.5468897819519043\nMinibatch accuracy: 12.5%\nValidation accuracy: 10.0%\nMinibatch loss at step 50: 2.2210307121276855\nMinibatch accuracy: 18.8%\nValidation accuracy: 20.6%\nMinibatch loss at step 100: 2.179996967315674\nMinibatch accuracy: 18.8%\nValidation accuracy: 31.3%\nMinibatch loss at step 150: 2.063338279724121\nMinibatch accuracy: 37.5%\nValidation accuracy: 41.2%\nMinibatch loss at step 200: 1.701377272605896\nMinibatch accuracy: 68.8%\nValidation accuracy: 48.9%\nMinibatch loss at step 250: 1.7243231534957886\nMinibatch accuracy: 43.8%\nValidation accuracy: 54.4%\nMinibatch loss at step 300: 1.4650031328201294\nMinibatch accuracy: 62.5%\nValidation accuracy: 62.5%\nMinibatch loss at step 350: 1.0505726337432861\nMinibatch accuracy: 68.8%\nValidation accuracy: 65.7%\nMinibatch loss at step 400: 0.9026930928230286\nMinibatch accuracy: 75.0%\nValidation accuracy: 70.5%\nMinibatch loss at step 450: 0.8984693288803101\nMinibatch accuracy: 75.0%\nValidation accuracy: 72.3%\nMinibatch loss at step 500: 1.1741290092468262\nMinibatch accuracy: 75.0%\nValidation accuracy: 73.9%\nMinibatch loss at step 550: 0.6169742345809937\nMinibatch accuracy: 87.5%\nValidation accuracy: 74.5%\nMinibatch loss at step 600: 0.9071530103683472\nMinibatch accuracy: 75.0%\nValidation accuracy: 77.5%\nMinibatch loss at step 650: 1.1652218103408813\nMinibatch accuracy: 87.5%\nValidation accuracy: 78.1%\nMinibatch loss at step 700: 0.6488093733787537\nMinibatch accuracy: 87.5%\nValidation accuracy: 78.0%\nMinibatch loss at step 750: 0.9689077734947205\nMinibatch accuracy: 62.5%\nValidation accuracy: 77.3%\nMinibatch loss at step 800: 0.9563777446746826\nMinibatch accuracy: 68.8%\nValidation accuracy: 78.7%\nMinibatch loss at step 850: 0.1593075394630432\nMinibatch accuracy: 93.8%\nValidation accuracy: 78.9%\nMinibatch loss at step 900: 1.6396489143371582\nMinibatch accuracy: 62.5%\nValidation accuracy: 79.0%\nMinibatch loss at step 950: 0.5512230396270752\nMinibatch accuracy: 87.5%\nValidation accuracy: 79.5%\nMinibatch loss at step 1000: 0.8293801546096802\nMinibatch accuracy: 75.0%\nValidation accuracy: 79.9%\nTest accuracy: 86.0%\n"
    }
   ],
   "source": [
    "train(graph_pool, optimizer, train_prediction, valid_prediction, test_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klf21gpbAgb-"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a convolutional net. Look for example at the classic [LeNet5](http://yann.lecun.com/exdb/lenet/) architecture, adding Dropout, and/or adding learning rate decay.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(200000, 32, 32, 1)\n(10000, 32, 32, 1)\n(10000, 32, 32, 1)\n"
    }
   ],
   "source": [
    "# Convert images from size of 28x28 to 32x32 as per LeNet specification.\n",
    "train_dataset = np.pad(train_dataset, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "valid_dataset = np.pad(valid_dataset, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "test_dataset = np.pad(test_dataset, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "print(train_dataset.shape)\n",
    "print(valid_dataset.shape)\n",
    "print(test_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "image_size = 32\n",
    "\n",
    "conv_1_filter_size = 5\n",
    "conv_1_filter_count = 20\n",
    "conv_1_stride = 1\n",
    "\n",
    "conv_2_filter_size = 5\n",
    "conv_2_filter_count = 50\n",
    "conv_2_stride = 1\n",
    "\n",
    "flattened_size = 5 * 5 * 50\n",
    "dense_1_count = 500\n",
    "dense_2_count = 84\n",
    "\n",
    "lenet_5 = tf.Graph()\n",
    "\n",
    "with lenet_5.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "  # Convolutional layer 1\n",
    "  w_conv_1 = weight_variable([conv_1_filter_size, conv_1_filter_size, 1, conv_1_filter_count])\n",
    "  bias_conv_1 = bias_variable([conv_1_filter_count])\n",
    "\n",
    "  # Convolutional layer 2\n",
    "  w_conv_2 = weight_variable([conv_2_filter_size, conv_2_filter_size, conv_1_filter_count, conv_2_filter_count])\n",
    "  bias_conv_2 = bias_variable([conv_2_filter_count])\n",
    "\n",
    "  # Dense layer 1\n",
    "  w_dense_1 = weight_variable([flattened_size, dense_1_count])\n",
    "  bias_dense_1 = bias_variable([dense_1_count])\n",
    "\n",
    "  # Dense layer 2\n",
    "  w_dense_2 = weight_variable([dense_1_count, dense_2_count])\n",
    "  bias_dense_2 = bias_variable([dense_2_count])\n",
    "\n",
    "  # Output layer\n",
    "  w_output = weight_variable([dense_2_count, num_labels])\n",
    "  bias_output = bias_variable([num_labels])\n",
    "\n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv_1_out = tf.nn.relu(tf.nn.conv2d(data, w_conv_1, strides=[1, 1, 1, 1], padding='VALID') + bias_conv_1)\n",
    "    # Tried also with avg_pool but max_pool performance tends to be better\n",
    "    pool_1_out = tf.nn.max_pool(conv_1_out, [1, 2, 2, 1], [1, 2, 2, 1], 'VALID')\n",
    "    conv_2_out = tf.nn.relu(tf.nn.conv2d(pool_1_out, w_conv_2, strides=[1, 1, 1, 1], padding='VALID') + bias_conv_2)\n",
    "    pool_2_out = tf.nn.max_pool(conv_2_out, [1, 2, 2, 1], [1, 2, 2, 1], 'VALID')\n",
    "    flat = tf.reshape(pool_2_out, [-1, flattened_size])\n",
    "    dense_1_out = tf.nn.relu(tf.matmul(flat, w_dense_1) + bias_dense_1)\n",
    "    dropout_1 = tf.nn.dropout(dense_1_out, rate=0.5)\n",
    "    dense_2_out = tf.nn.relu(tf.matmul(dropout_1, w_dense_2) + bias_dense_2)    \n",
    "    dropout_2 = tf.nn.dropout(dense_2_out, rate=0.5)\n",
    "    logits = tf.matmul(dropout_2, w_output) + bias_output\n",
    "    return logits\n",
    "\n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = compute_loss(tf_train_labels, logits)\n",
    "\n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.AdamOptimizer(learning_rate=0.0008).minimize(loss)\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Initialized\nMinibatch loss at step 0: 4.662407875061035\nMinibatch accuracy: 10.2%\nValidation accuracy: 11.3%\nMinibatch loss at step 50: 1.237686038017273\nMinibatch accuracy: 59.0%\nValidation accuracy: 58.8%\nMinibatch loss at step 100: 1.0148578882217407\nMinibatch accuracy: 70.7%\nValidation accuracy: 74.6%\nMinibatch loss at step 150: 0.7051034569740295\nMinibatch accuracy: 80.9%\nValidation accuracy: 79.3%\nMinibatch loss at step 200: 0.706531286239624\nMinibatch accuracy: 77.7%\nValidation accuracy: 80.7%\nMinibatch loss at step 250: 0.5412888526916504\nMinibatch accuracy: 84.4%\nValidation accuracy: 81.7%\nMinibatch loss at step 300: 0.5233900547027588\nMinibatch accuracy: 82.4%\nValidation accuracy: 83.3%\nMinibatch loss at step 350: 0.5371547937393188\nMinibatch accuracy: 83.6%\nValidation accuracy: 83.8%\nMinibatch loss at step 400: 0.5158592462539673\nMinibatch accuracy: 83.6%\nValidation accuracy: 84.5%\nMinibatch loss at step 450: 0.6056300401687622\nMinibatch accuracy: 82.8%\nValidation accuracy: 85.1%\nMinibatch loss at step 500: 0.5347809791564941\nMinibatch accuracy: 84.8%\nValidation accuracy: 85.1%\nMinibatch loss at step 550: 0.49956026673316956\nMinibatch accuracy: 85.9%\nValidation accuracy: 85.7%\nMinibatch loss at step 600: 0.5128731727600098\nMinibatch accuracy: 84.8%\nValidation accuracy: 85.7%\nMinibatch loss at step 650: 0.4744938015937805\nMinibatch accuracy: 85.9%\nValidation accuracy: 85.9%\nMinibatch loss at step 700: 0.4453263282775879\nMinibatch accuracy: 87.9%\nValidation accuracy: 86.5%\nMinibatch loss at step 750: 0.42846643924713135\nMinibatch accuracy: 84.8%\nValidation accuracy: 86.5%\nMinibatch loss at step 800: 0.35436585545539856\nMinibatch accuracy: 90.2%\nValidation accuracy: 86.7%\nMinibatch loss at step 850: 0.453427791595459\nMinibatch accuracy: 85.2%\nValidation accuracy: 86.6%\nMinibatch loss at step 900: 0.45648109912872314\nMinibatch accuracy: 84.4%\nValidation accuracy: 87.1%\nMinibatch loss at step 950: 0.43254518508911133\nMinibatch accuracy: 87.1%\nValidation accuracy: 87.6%\nMinibatch loss at step 1000: 0.5107581615447998\nMinibatch accuracy: 85.9%\nValidation accuracy: 87.9%\nTest accuracy: 93.1%\n"
    }
   ],
   "source": [
    "train(lenet_5, optimizer, train_prediction, valid_prediction, test_prediction)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "cnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}